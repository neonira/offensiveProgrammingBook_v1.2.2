[
["index.html", "Offensive Programming Book Welcome Book version Book production", " Offensive Programming Book Fabien GELINEAU neonira@gmail.com 2020-Q1 Welcome This is the website dedicated to offensive programming. This digital book will teach you how to put offensive programming in action with R. This website is and will remain free to use. It is licensed under the Creative Commons Attribution-NonCommercial-NoDerivs 3.0 License. Book version This book version is 1.2.2 and it relates to following R package versions, available on CRAN R package goal version wyz.code.offensiveProgramming offensive programming core functions 1.1.17 or higher wyz.code.testthat automated generation of testthat code for offensive programming instrumented functions 1.1.16 or higher wyz.code.metaTesting automated generation of data for testing of offensive programming instrumented functions 1.1.11 or higher wyz.code.rdoc R documentation generation functions and API 1.1.15 Book production The book is written in RMarkdown with bookdown. You may get access to the source for contributions. The book is for the moment just resulting of my efforts. Hope it will be enhanced and result from a collaborative effort very soon. This book is identified by ISBN 979-10-699-4075-8. "],
["preamble.html", "Preamble", " Preamble Package wyz.code.offensiveProgramming aims to provide a strict type checking enforcement in R. The R language is a weakly typed script language. As such, it simplifies greatly program writing and allows for great flexibility. That’s fine, and conceptually, there is no issue about that. Indeed, this leads more than desired to defensive programming practice. The absolute need to verify provided arguments, all along the function call chain, is very strong if you want to provide reliable and robust implementation. Consider following table about benefits of type control and type inference in lazy and strict type checking approaches. Goal Lazy type control standard R Strict type checking R offensive programming type control Weak type control implies coding of many contextual type controls Strong type checking brings some rigidity type inference requires knowledge and navigation from code to doc forth and back is required semantic naming allows more intuitive type inference Won’t it be nice to get the benefits of defensive programming and offensive programming, where ever and when ever needed? The main objective of package wyz.code.offensiveProgramming is to allow strict type checking in R to be as easy to use and to run as standard R. "],
["programming-styles.html", " 1 Programming styles 1.1 Defensive programming style 1.2 Offensive programming style", " 1 Programming styles General reference might be found on wikiwand for the one who would like to get a contextual introduction. 1.1 Defensive programming style Defensive programming style consist of implementation of many controls at the entry of functions. This is required to ensure parameters meet some conditions required by the developer or the context. There are some benefits to this style. It is clearly incremental, and you can always add as many controls as desired, to meet the level of robustness and reliability you aim for. Indeed, this comes with some drawbacks that are repeated verifications at various depth levels, and worst the executed verifications are often the same code, leading to a waste of time during implementation, testing and execution. Second, an evolution of any function signature in the function call chain, may imply shallow or deep changes in the subsequent verifications, depending of the case. Defensive programming style also requires a good documentation, as there is no way to infer rationally and with a high level of confidence, what type has to be used for a given parameter of a function. Notice also that quality and consistency of verification for types and values changes deeply from one package to another, from one developer to another. There exists a great variability of quality and packages users have to deal with it. 1.2 Offensive programming style Offensive programming styles works differently. It is based on an implicit contract with bilateral responsibilities developer implements required verification about values, not about types unless absolutely required, end-user provides values with right or wrong types, when using developer delivered package type checks can be enforced at any time by end-user if needed. They are generally not hard coded by the developer in each function. In this way, the contract is end-user is responsible of type concordance developer is responsible of implementation from type concordance. Instrumentation is provided to let the end-user easily check the type concordance, whenever and wherever needed. "],
["application-in-r-language.html", " 2 Application in R language 2.1 A simple case 2.2 Defensive programming 2.3 Offensive programming 2.4 Back to definition", " 2 Application in R language 2.1 A simple case Consider following R class implementation which provides some basic mathematics operations. MathOperation &lt;- function() { self &lt;- environment() class(self) &lt;- append(&#39;Addition&#39;, class(self)) add &lt;- function(x, y) x + y multiply &lt;- function(x, y) x * y divide &lt;- function(x, y) x / y self } Let’s not argue about the design and relevancy of the approach. Instead, can you tell the scopes of each function, and identify/inventorize the implementations flaws ? 2.2 Defensive programming In standard R, the provided implementation might behave correctly, erroneously, or even generate errors, depending on the inputs you provide. Is it bad code ? Not at all according to me. The class name mentions clearly the intent that is to encapsulate some math operations. There are 3 operations. They can take any argument that can be accepted by operators ‘+’, ’*’ or ‘/’. So, providing, integers, doubles, and complex numbers should work. If you use an external package like gmp, it is also an acceptable input for any of the needed parameters. Any combination of this types will provide a correct result, using scalars or vectors. From my point of view, main issues are the followings issue number issue description issue severity 1 few seconds for creation, several quarters of an hour for testing, and hours for documentation UNACCEPTABLE 2 does it complies with maths sets? Not at all, this is software engineering implementation, not a math compliant one SEVERE 3 high sensitivity to input values did you consider that NaN, NA, Inf, -Inf, 0 could be valid input values here?. Indeed R is naturally great on this part LOW 4 natural polymorphism of returned types, that brings again software engineering whereas reliable math ops are needed. From a mathematical point of view, input belong to a predefined mathematical set, and output belongs also to a predefined mathematical set. Not the case with provided implementations HIGH 5 unreliable implementation as input might return numeric output, warning or errors HIGH 2.3 Offensive programming Consider same R class implementation with a little bit instrumentation. suppressMessages(require(data.table)) MathOperation &lt;- function() { self &lt;- environment() class(self) &lt;- append(&#39;Addition&#39;, class(self)) add &lt;- function(x_r, y_r) x_r + y_r multiply &lt;- function(x_r, y_r) x_r * y_r divide &lt;- function(x_r, y_r) x_r / y_r function_return_types &lt;- data.table( function_name = c(&#39;add&#39;, &#39;multiply&#39;, &#39;divide&#39;), return_value = c(&#39;x_r&#39;, &#39;x_r&#39;, &#39;x_d&#39;) ) self } 2.3.1 What are the differences? Compare to previously shown implementation, here are the two main differences arguments are renamed according to a pattern a variable named function_return_types has been added. It holds a data.table that defines expected function return types. That’s it. Function implementation is exactly the same. No change done elsewhere. Everything is there and should be sufficient to solve many of the faced issues. 2.3.2 Semantic argument naming Arguments have been renamed from x to x_r. What does that mean? Syntactically, it changes nothing for R. For us humans, it changes a lot of things, as this follows a pattern that allows to specify several intents in a short, concise, and reliable way. The pattern is simple to understand. Refer to 5 to know more about syntax, and to discover many illustrative examples. 2.4 Back to definition So now, you know the variable x_r is just a vector of real values, unconstrained in length. Using this parameter name implies that the the developer is responsible for testing cases of various length and has to prevent weirdness propagation. For example following R code shows results that require decisions mo &lt;- MathOperation() print(mo$add(1.0 * 1:3, 1.0 * 1:7)) #&gt; Warning in x_r + y_r: longer object length is not a multiple of shorter object #&gt; length #&gt; [1] 2 4 6 5 7 9 8 This code provides both an output and a warning, because of R recycling on various length vectors. What decision should be taken ? Allow or deny this behavior ? It depends of your usage. If you are creating a real math library, I would recommend to duplicate the code and create two functions named addRCompliant and addMathCompliant. Later should enforce arguments length control in his body, while former should keep the body as is or instrument it with an encapsulating suppressWarning call. That way, you should easily meet your end-users expectations, either mathematicians or software engineers. Note that in the later case, added controls are not defensive programming but functional scope verification. "],
["activating-offensive-programming.html", " 3 Activating offensive programming 3.1 Package discovery", " 3 Activating offensive programming Package wyz.code.offensiveProgramming provides R tools to ease offensive programming exploitation. It aims to provide all the core tools required to do so. Package wyz.code.testthat uses meta programming to generate testthat test files, you will use as if they had been written manually. Package wyz.code.metaTesting exploits offensive programming semantic names to induce type parameters and to generate reusable values for tests, thus alleviating the burden to produce data for testing purposes. Package wyz.code.rdoc provides an API to ease documentation generation in a programmatic way. This is the only package that does not requires offensive programming and that can be used with plain standard R code. 3.1 Package discovery These packages come with manual pages you could refer to if needed. Indeed, there exists a smarter way to proceed as each of these packages provides a self describing function named op.*Information. It provides tabular information as a data.table that can easily be sorted and filtered out to match your own search criteria. This is helpful daily as a reminder or as an help to ease discovery and to get quick insight about content. Let’s see a typical R hands-on session oi &lt;- opInformation() oi[category == &#39;CLASS&#39;, . (name, nature)] #&gt; name nature #&gt; 1: EvaluationMode EXPORTED #&gt; 2: FunctionParameterName EXPORTED #&gt; 3: FunctionParameterTypeFactory EXPORTED #&gt; 4: TestCaseDefinition EXPORTED oi[category == &#39;FUNCTION&#39;, . (name, nature)] #&gt; name nature #&gt; 1: defineEvaluationModes EXPORTED #&gt; 2: defineFunctionReturnTypesParameterName EXPORTED #&gt; 3: defineTestCaseDefinitionsParameterName EXPORTED #&gt; 4: exploreObjectNamesVerification EXPORTED #&gt; 5: findFilesInPackage EXPORTED #&gt; 6: generateStatusSummary INTERNAL #&gt; 7: getClassTypicalFunctionNames INTERNAL #&gt; 8: getEllipsisName EXPORTED #&gt; 9: getObjectClassKind EXPORTED #&gt; 10: getObjectClassNames EXPORTED #&gt; 11: getObjectFunctionArgumentNames EXPORTED #&gt; 12: getObjectFunctionNames EXPORTED #&gt; 13: hasMainClass INTERNAL #&gt; 14: identifyOPInstrumentationLevel EXPORTED #&gt; 15: isAuditable EXPORTED #&gt; 16: matchFunctionArguments INTERNAL #&gt; 17: matchFunctionSignature EXPORTED #&gt; 18: opInformation EXPORTED #&gt; 19: retrieveFactory EXPORTED #&gt; 20: retrieveFunctionArgumentNames EXPORTED #&gt; 21: retrieveFunctionArguments EXPORTED #&gt; 22: retrieveFunctionReturnTypes EXPORTED #&gt; 23: retrievePackageFunctionNames EXPORTED #&gt; 24: retrieveSupportedObjectInformation INTERNAL #&gt; 25: retrieveTestCaseDefinitions EXPORTED #&gt; 26: retrieveTestCaseDescriptions EXPORTED #&gt; 27: runFunction EXPORTED #&gt; 28: runTestCase EXPORTED #&gt; 29: runTransientFunction EXPORTED #&gt; 30: verifyClassName EXPORTED #&gt; 31: verifyFunctionArguments EXPORTED #&gt; 32: verifyFunctionName EXPORTED #&gt; 33: verifyFunctionReturnTypesDefinition EXPORTED #&gt; 34: verifyName INTERNAL #&gt; 35: verifyObjectNames EXPORTED #&gt; 36: verifyTestCaseDefinitions EXPORTED #&gt; name nature "],
["the-type-factory.html", " 4 The type factory 4.1 Get recorded types inventory 4.2 Understanding the model 4.3 Register your own type 4.4 Get access to verification functions 4.5 Some hints 4.6 Enforcing use of your own type factory", " 4 The type factory The types that you wish to control are managed by a type factory. Retrieveing an object of this class, using function retrieveFactory, allows you to discover what are the already recorded types, available for reuse register your own types so they can be checked dynamically wherever and whenever required understand the verification logic for each type 4.1 Get recorded types inventory Simply use getRecordedTypes function. It returns a data.table, that you can filter out conveniently. Here is an example. f &lt;- retrieveFactory() f$getRecordedTypes() #&gt; suffix type verify_function category #&gt; 1: a array &lt;function&gt; data structure #&gt; 2: b boolean &lt;function&gt; math #&gt; 3: c complex &lt;function&gt; numeric #&gt; 4: ca call &lt;function&gt; language #&gt; 5: ch character &lt;function&gt; basic #&gt; 6: cm complex-math &lt;function&gt; math #&gt; 7: d double &lt;function&gt; numeric #&gt; 8: da date &lt;function&gt; date #&gt; 9: dc POSIXct &lt;function&gt; date #&gt; 10: df data.frame &lt;function&gt; data structure #&gt; 11: dl POSIXlt &lt;function&gt; date #&gt; 12: dm double-math &lt;function&gt; math #&gt; 13: dt data.table &lt;function&gt; data structure #&gt; 14: e environment &lt;function&gt; basic #&gt; 15: er error &lt;function&gt; error management #&gt; 16: ex expression &lt;function&gt; language #&gt; 17: f function &lt;function&gt; basic #&gt; 18: fa factor &lt;function&gt; basic #&gt; 19: i integer &lt;function&gt; numeric #&gt; 20: im integer-math &lt;function&gt; math #&gt; 21: l list &lt;function&gt; data structure #&gt; 22: lo logical &lt;function&gt; basic #&gt; 23: m matrix &lt;function&gt; data structure #&gt; 24: n numeric &lt;function&gt; numeric #&gt; 25: na na &lt;function&gt; basic #&gt; 26: ni negative integer &lt;function&gt; math #&gt; 27: nm name &lt;function&gt; language #&gt; 28: nr negative real &lt;function&gt; math #&gt; 29: o object &lt;function&gt; basic #&gt; 30: pi positive integer &lt;function&gt; math #&gt; 31: pr positive real &lt;function&gt; math #&gt; 32: r real-math &lt;function&gt; math #&gt; 33: ra raw &lt;function&gt; basic #&gt; 34: rm real-math alias &lt;function&gt; math #&gt; 35: s string &lt;function&gt; basic #&gt; 36: sni strictly negative integer &lt;function&gt; math #&gt; 37: snr strictly negative real &lt;function&gt; math #&gt; 38: spi strictly positive integer &lt;function&gt; math #&gt; 39: spr strictly positive real &lt;function&gt; math #&gt; 40: t table &lt;function&gt; data structure #&gt; 41: ui unsigned integer &lt;function&gt; math #&gt; 42: ur unsigned real &lt;function&gt; math #&gt; 43: w warning &lt;function&gt; error management #&gt; suffix type verify_function category 4.2 Understanding the model A type is defined by three elements a unique suffix a unique name a function that returns a boolean value, TRUE when examining a value that matches the type For example, you might wonder what means ui as a suffix f$getRecordedTypes()[suffix == &#39;ui&#39;] #&gt; suffix type verify_function category #&gt; 1: ui unsigned integer &lt;function&gt; math What is the function related to suffix ui? f$getVerificationFunction(&#39;ui&#39;) #&gt; function(o_1l_) isPureMathInteger(o_1l_) &amp;&amp; all(o_1l_ &gt;= 0L) #&gt; &lt;bytecode: 0x557d9bd328d8&gt; #&gt; &lt;environment: 0x557d9bdb37d8&gt; 4.3 Register your own type Type registration is achieved by providing a type suffix, a type name and a verification function. Registration will be tagged automatically as user-defined. Here is a typical sequence to register your own type f$addSuffix(&#39;mc&#39;, &#39;MyClass&#39;, function(o_1l_) is(o_1l_, &#39;MyClass&#39;)) #&gt; [1] TRUE f$getRecordedTypes()[suffix == &#39;mc&#39;] #&gt; suffix type verify_function category #&gt; 1: mc MyClass &lt;function&gt; user defined Note that no implementation of the class is required. It is purely declarative registration. You told the type factory to record the type MyClass under the suffix mc, with the verification function you provided. Here verification function is quite simple. Notice that it takes a single object as argument and that function signature must match for the operation to succeed. Always bear great attention to returned value. It must be TRUE for your type to be registered into the factory. 4.4 Get access to verification functions Implementation of verification function can range from quite simple to as complex as required. This allows you to manage functional scopes much more easily, whatever you work and organization context. For example, if you want to see at a glance the differences between a boolean and a logical, here is the sequence you could use f$getVerificationFunction(&#39;b&#39;) #&gt; function(o_1l_) { #&gt; if (!is.logical(o_1l_)) return(FALSE) #&gt; if (length(o_1l_) == 0) return(TRUE) #&gt; all(is.na(o_1l_) == FALSE) #&gt; } #&gt; &lt;bytecode: 0x557d9bd10e50&gt; #&gt; &lt;environment: 0x557d9bdb37d8&gt; f$getVerificationFunction(&#39;logical&#39;) #&gt; function (x) .Primitive(&quot;is.logical&quot;) From the two definitions, you can deduce the differences between the two types. A boolean is a 2-value boolean, either TRUE or FALSE. A logical, is a R logical value, that is a 3-value boolean, so it may take value NA. Note that arguments to the function getVerificationFunction can be either a registered suffix or a registered type, as shown on this example. 4.5 Some hints There is currently no way to remove one recorded type. This need is indeed very specific and arise only when there is a name collision and you wish to use an already taken name for your own purpose. Solution is quite simple, use another name. Provided types are the most commons, and the current suffixes have been chosen for ease of use and for intuitive usage. Whenever you need to register a new type, ask yourself ‘what is the suffix I wish to use for the new type?’. My advice is to use short suffixes, made of 2 or 3 letters. That’s clearly sufficient to distinguish your type from others. Know that there is not limitation to the length of the suffix you can use. Simply, comply with KISS, as you will have to type it several times, probably. If you come from another programming language, you may consider to create aliased types by recording new entries. Let’s look at a concrete case. f$addSuffix(&#39;ui&#39;, &#39;unsigned integer&#39;, function(o_1_) f$getVerificationFunction(&#39;i&#39;)(o_1l_) &amp;&amp; o_1_ &gt;= 0L) #&gt; [1] FALSE f$addSuffix(&#39;ul&#39;, &#39;unsigned long&#39; , f$getVerificationFunction(&#39;ui&#39;)) #&gt; [1] TRUE f$getRecordedTypes()[suffix %in% c(&#39;ui&#39;, &#39;ul&#39;)] #&gt; suffix type verify_function category #&gt; 1: ui unsigned integer &lt;function&gt; math #&gt; 2: ul unsigned long &lt;function&gt; user defined You asked to add two new entries in the type factory, and they share the same verification function. First add fails, second one succeeds. The reason is that ui suffix is already defined and you cannot redefine an already defined suffix. Now, ui and ul are aliases of the same verification function. Notice that this is true now, and cannot be changed once created. Indeed, you always have the opportunity to create a new type factory to match your new need. You can use as many type factories as you want. The term alias shall not be understood, as an authorization to use one name for the other, but rather as an ability to define quickly new types to ease functional scope management. 4.6 Enforcing use of your own type factory When you customized your own type factory, you need a way to tell wyz.code.offensiveProgramming to use it. To do so, simply create your type factory and assign it to a R variable, and set option op_type_factory to point to the name of this R variable. Let’s see an example ff &lt;- retrieveFactory() ff$addSuffix(&#39;wo&#39;, &quot;wo class&quot;, function(o_1l_) is(o_1l_, &quot;wo&quot;)) #&gt; [1] TRUE ff$addSuffix(&#39;yo&#39;, &quot;yo class&quot;, function(o_1l_) is(o_1l_, &quot;yo&quot;)) #&gt; [1] TRUE ff$addSuffix(&#39;zo&#39;, &quot;zo class&quot;, function(o_1l_) is(o_1l_, &quot;zo&quot;)) #&gt; [1] TRUE options(op_type_factory = ff) fg &lt;- retrieveFactory() # retrieves the factory pointed by R variable ff fg$getRecordedTypes()[suffix %in% c(&#39;wo&#39;, &#39;yo&#39;, &#39;zo&#39;)] # right behavior ! #&gt; suffix type verify_function category #&gt; 1: wo wo class &lt;function&gt; user defined #&gt; 2: yo yo class &lt;function&gt; user defined #&gt; 3: zo zo class &lt;function&gt; user defined # wrong behavior as retrieveFactory will provide the default factory and not yours! options(op_type_factory = &quot;&quot;) fh &lt;- retrieveFactory() # retrieves the default factory fh$getRecordedTypes()[suffix %in% c(&#39;wo&#39;, &#39;yo&#39;, &#39;zo&#39;)] #&gt; Empty data.table (0 rows and 4 cols): suffix,type,verify_function,category "],
["semantic-names.html", " 5 Semantic names 5.1 What is semantic naming? 5.2 Verifying names", " 5 Semantic names Package wyz.code.offensiveProgramming enforces semantic naming to offer great functionalities. Semantic naming come at the cost of few conditions. It has to be used for function parameters names function return type definition test case definitions 5.1 What is semantic naming? I call semantic naming, the fact that a named object used in the code should provide much more information than a dumb name. 5.1.1 Naming recommendation Naming conventions are recommendations. You may follow them strictly or lazily. This has some consequences on the checks you will have to run as you will have to know under which mode you work. Package wyz.code.offensiveProgramming defines a few naming recommendations to declare class, function, function parameters and code variable names. Table below shows them and exposes the underlying philosophy. name category philosophy pattern to comply with class starts with upper-cased letter, camel-cased [A-Z][a-ZA-Z0-9]* function starts with lower-cased letter, camel-cased [a-z][a-ZA-Z0-9]* function parameter See below next chapter code variable snake-cased [a-z]+([a-z]+)* 5.1.2 Semantic naming Semantic naming is required on function parameter names and on return type names. Each semantic name must comply with one of the following patterns. &lt;variableNameCamelCase&gt;_&lt;typeInformation&gt;_&lt;lengthConstraint&gt; &lt;variableNameCamelCase&gt;_&lt;lengthConstraint&gt;_ First pattern is to be used for monomorphic types. Second one for polymorphic types. Monomorphic types are types that are homogeneous. A string, a double, a MyObject are good example of such monomorphic types. Pattern allows to not only express concisely the type, but also to express some length constraints if needed. Polymorphic types are useful as soon as your input or output can take many types, according to your context. For example, a R function may return a double, or a warning, or an error. Polymorphic types always end with ’_’ in their names, to make them easy to identify. 5.1.2.1 Type part of semantic naming The type information part of the pattern has to match one suffix of the recorded entries of the type factory. 5.1.2.2 Length part of semantic naming The length constraint part of the pattern follows the PERL pattern ([1-9][0-9]*(l|m|n)?). Letters mean respectively less or equal, more or equal, 1 or n. The length constraint part is optional. 5.1.2.3 Examples of semantic names Look at following table to get more intuitive traction of semantic names. That’s easy to understand and to use. input name meaning x_s an unconstrained vector of strings - might contain no entries x_s_3 a vector of strings with 3 entries x_s_3l a vector of strings with 3 or less entries x_s_3m a vector of strings with 3 or more entries x_s_3n a vector of strings with 3 entries or 1 entry flag_b_1 a vector of booleans with 1 entry - a.k.a a boolean scalar z_ an unconstrained polymorphic type vector named z - Nothing more is known about its content z_2_ a polymorphic type vector named z of length 2 - Nothing more is known about its content It is often advised when programming to use meaningful variable names. That’s hold true with semantic naming, but is far less critical. Here I used x or z as variable names, without any sacrifice of semantic naming due to the patterns I used. Indeed, using more contextually meaningful names should have even been a greater approach. 5.2 Verifying names 5.2.1 Verifying a function parameter or return type declaration The function getTypeDescription from the type factory turns your semantic name into a human readable definition. It is a convenient way to verify your definition matches your need. It is also used to generate documentation for function parameter names. sapply(c(&#39;x_s_3n&#39;, &#39;flag_b_1&#39;, &#39;z_2_&#39;), function(e) f$getTypeDescription(FunctionParameterName(e))) #&gt; x_s_3n #&gt; &quot;A length-1 or 3 vector of string values&quot; #&gt; flag_b_1 #&gt; &quot;A single boolean value&quot; #&gt; z_2_ #&gt; &quot;A length-2 vector of variable type objects&quot; 5.2.2 Verifying a class name Simply use function verifyClassName. Remind to set parameter strict according to your compliance to semantic naming. Use TRUE for strict compliance, FALSE for mazy compliance. verifyClassName(c(&#39;alphaBeta&#39;, &#39;AlphaBeta&#39;, &#39;.alphaBeta&#39;)) #&gt; alphaBeta AlphaBeta .alphaBeta #&gt; FALSE TRUE FALSE verifyClassName(c(&#39;alphaBeta&#39;, &#39;AlphaBeta&#39;, &#39;.alphaBeta&#39;), strict = FALSE) #&gt; alphaBeta AlphaBeta .alphaBeta #&gt; TRUE TRUE TRUE 5.2.3 Verifying a function name Simply use function verifyFunctionName. Remind to set parameter strict according to your compliance to semantic naming. Use TRUE for strict compliance, FALSE for mazy compliance. verifyFunctionName(c(&#39;alphaBetaGamma&#39;, &#39;AlphaBetaGamma&#39;, &#39;.alphaBetaGamma&#39;)) #&gt; alphaBetaGamma AlphaBetaGamma .alphaBetaGamma #&gt; TRUE FALSE FALSE verifyFunctionName(c(&#39;alphaBetaGamma&#39;, &#39;AlphaBetaGamma&#39;, &#39;.alphaBetaGamma&#39;), strict = FALSE) #&gt; alphaBetaGamma AlphaBetaGamma .alphaBetaGamma #&gt; TRUE TRUE TRUE 5.2.4 Get naming balance from an R object Use function verifyObjectNames to get the results of a full semantic naming compliance analysis of names for the provided object. This analysis tries both strict and lazy compliance to provide results. source(system.file(&#39;code-samples/no-defs/Addition.R&#39;, package = &#39;wyz.code.offensiveProgramming&#39;)) verifyObjectNames(Addition()) #&gt; $class_name_compliance #&gt; Addition #&gt; TRUE #&gt; #&gt; $function_name_compliance #&gt; addDouble addInteger addNumeric divideByZero generateError #&gt; TRUE TRUE TRUE TRUE TRUE #&gt; generateWarning #&gt; TRUE #&gt; #&gt; $parameter_name_compliance #&gt; function_name parameter_name name_compliance_check semantic_naming_check #&gt; 1: addDouble x_d TRUE TRUE #&gt; 2: addDouble y_d TRUE TRUE #&gt; 3: addInteger x_i TRUE TRUE #&gt; 4: addInteger y_i TRUE TRUE #&gt; 5: addNumeric x FALSE FALSE #&gt; 6: addNumeric y_n TRUE TRUE #&gt; 7: divideByZero x_n TRUE TRUE #&gt; 8: generateError &lt;NA&gt; TRUE TRUE #&gt; 9: generateWarning x_ TRUE TRUE #&gt; #&gt; $classname #&gt; [1] &quot;Addition&quot; #&gt; #&gt; $owns_function_return_type_information #&gt; [1] FALSE #&gt; #&gt; $owns_test_case_definitions #&gt; [1] FALSE #&gt; #&gt; $supports_strict_compliance #&gt; [1] FALSE #&gt; #&gt; $supports_lazy_compliance #&gt; [1] FALSE #&gt; #&gt; $can_be_typed_checked #&gt; [1] FALSE #&gt; #&gt; $is_function_fully_instrumented #&gt; [1] FALSE #&gt; #&gt; $missing_functions #&gt; [1] NA #&gt; #&gt; $is_test_case_fully_instrumented #&gt; [1] FALSE #&gt; #&gt; $missing_test_cases #&gt; [1] NA #&gt; #&gt; $sof #&gt; $sof$frt #&gt; [1] FALSE #&gt; #&gt; $sof$tcd #&gt; [1] FALSE #&gt; #&gt; $sof$instrumented_fn #&gt; NULL #&gt; #&gt; $sof$instrumented_tc #&gt; NULL source(system.file(&#39;code-samples/both-defs/good/full/AdditionTCFIG1.R&#39;, package = &#39;wyz.code.offensiveProgramming&#39;)) verifyObjectNames(AdditionTCFIG1()) #&gt; $class_name_compliance #&gt; AdditionTCFIG1 #&gt; TRUE #&gt; #&gt; $function_name_compliance #&gt; addDouble addInteger addMultiDouble addMultiInteger divideByZero #&gt; TRUE TRUE TRUE TRUE TRUE #&gt; generateError generateWarning #&gt; TRUE TRUE #&gt; #&gt; $parameter_name_compliance #&gt; function_name parameter_name name_compliance_check semantic_naming_check #&gt; 1: addDouble x_d TRUE TRUE #&gt; 2: addDouble y_d TRUE TRUE #&gt; 3: addInteger x_i TRUE TRUE #&gt; 4: addInteger y_i TRUE TRUE #&gt; 5: addMultiDouble ... TRUE TRUE #&gt; 6: addMultiInteger x_i TRUE TRUE #&gt; 7: addMultiInteger ... TRUE TRUE #&gt; 8: divideByZero x_n TRUE TRUE #&gt; 9: generateError &lt;NA&gt; TRUE TRUE #&gt; 10: generateWarning x_ TRUE TRUE #&gt; #&gt; $classname #&gt; [1] &quot;AdditionTCFIG1&quot; #&gt; #&gt; $owns_function_return_type_information #&gt; [1] TRUE #&gt; #&gt; $owns_test_case_definitions #&gt; [1] TRUE #&gt; #&gt; $supports_strict_compliance #&gt; [1] TRUE #&gt; #&gt; $supports_lazy_compliance #&gt; [1] TRUE #&gt; #&gt; $can_be_typed_checked #&gt; [1] TRUE #&gt; #&gt; $is_function_fully_instrumented #&gt; [1] TRUE #&gt; #&gt; $missing_functions #&gt; [1] &quot;none&quot; #&gt; #&gt; $is_test_case_fully_instrumented #&gt; [1] TRUE #&gt; #&gt; $missing_test_cases #&gt; [1] &quot;none&quot; #&gt; #&gt; $sof #&gt; $sof$frt #&gt; [1] TRUE #&gt; #&gt; $sof$tcd #&gt; [1] TRUE #&gt; #&gt; $sof$instrumented_fn #&gt; function_name return_value #&gt; 1: addDouble x_d #&gt; 2: addInteger x_i #&gt; 3: addMultiDouble x_d #&gt; 4: divideByZero x_d #&gt; 5: addMultiInteger x_i #&gt; 6: generateWarning x_w #&gt; 7: generateError x_er #&gt; #&gt; $sof$instrumented_tc #&gt; function_name standard_evaluation type_checking_enforcement #&gt; 1: addDouble correct correct #&gt; 2: addDouble correct correct #&gt; 3: addDouble correct correct #&gt; 4: addDouble correct correct #&gt; 5: addDouble correct correct #&gt; 6: addDouble erroneous erroneous #&gt; 7: addDouble correct failure #&gt; 8: addDouble correct failure #&gt; 9: addDouble correct failure #&gt; 10: addInteger correct correct #&gt; 11: addInteger correct correct #&gt; 12: addInteger correct correct #&gt; 13: addInteger correct correct #&gt; 14: addInteger erroneous failure #&gt; 15: addInteger correct failure #&gt; 16: addInteger correct failure #&gt; 17: addInteger correct failure #&gt; 18: addInteger correct failure #&gt; 19: divideByZero correct correct #&gt; 20: divideByZero correct correct #&gt; 21: divideByZero correct correct #&gt; 22: generateWarning correct correct #&gt; 23: generateError failure failure #&gt; 24: addMultiDouble correct correct #&gt; 25: addMultiDouble correct correct #&gt; 26: addMultiDouble correct correct #&gt; 27: addMultiInteger correct correct #&gt; 28: addMultiInteger correct failure #&gt; 29: addMultiInteger correct correct #&gt; function_name standard_evaluation type_checking_enforcement #&gt; test_case #&gt; 1: &lt;TestCaseDefinition&gt; #&gt; 2: &lt;TestCaseDefinition&gt; #&gt; 3: &lt;TestCaseDefinition&gt; #&gt; 4: &lt;TestCaseDefinition&gt; #&gt; 5: &lt;TestCaseDefinition&gt; #&gt; 6: &lt;TestCaseDefinition&gt; #&gt; 7: &lt;TestCaseDefinition&gt; #&gt; 8: &lt;TestCaseDefinition&gt; #&gt; 9: &lt;TestCaseDefinition&gt; #&gt; 10: &lt;TestCaseDefinition&gt; #&gt; 11: &lt;TestCaseDefinition&gt; #&gt; 12: &lt;TestCaseDefinition&gt; #&gt; 13: &lt;TestCaseDefinition&gt; #&gt; 14: &lt;TestCaseDefinition&gt; #&gt; 15: &lt;TestCaseDefinition&gt; #&gt; 16: &lt;TestCaseDefinition&gt; #&gt; 17: &lt;TestCaseDefinition&gt; #&gt; 18: &lt;TestCaseDefinition&gt; #&gt; 19: &lt;TestCaseDefinition&gt; #&gt; 20: &lt;TestCaseDefinition&gt; #&gt; 21: &lt;TestCaseDefinition&gt; #&gt; 22: &lt;TestCaseDefinition&gt; #&gt; 23: &lt;TestCaseDefinition&gt; #&gt; 24: &lt;TestCaseDefinition&gt; #&gt; 25: &lt;TestCaseDefinition&gt; #&gt; 26: &lt;TestCaseDefinition&gt; #&gt; 27: &lt;TestCaseDefinition&gt; #&gt; 28: &lt;TestCaseDefinition&gt; #&gt; 29: &lt;TestCaseDefinition&gt; #&gt; test_case "],
["evaluation-modes.html", " 6 Evaluation modes 6.1 Understanding evaluation modes 6.2 Instantiating evaluation mode", " 6 Evaluation modes Package wyz.code.offensiveProgramming comes with several evaluation modes. Evaluation modes allow you to set up a particular execution context according to our needs. This allows to run R function under the given evaluation context, and to get back an evaluation mode compliance analysis result. You may retrieve evaluation modes by using function defineEvaluationModes that returns the 3 following modes print(defineEvaluationModes()) #&gt; [1] &quot;standard_R_evaluation&quot; &quot;enhanced_R_evaluation&quot; #&gt; [3] &quot;type_checking_enforcement&quot; 6.1 Understanding evaluation modes The first mode, standard_R_evaluation, is to ease comparisons with standard R evaluation. It does not make any sense to use only this mode when using wyz.code.offensiveProgramming. The second mode, enhanced_R_evaluation, goes further than standard R evaluation, as it implies a function return type verification. This mode will check your function returns values in accordance with the declared returned type. This mode let you add to R a way to declared expected return type for a function. The third mode, type_checking_enforcement, goes still further than the second mode, as it implies a function parameter types verification. This mode will check that each parameter provided to the function will match its semantic name defined type. This mode mimics a compiler output for R but it still interpreted language. 6.2 Instantiating evaluation mode To handle evaluation mode, use function EvaluationMode. A typical way to do so is em &lt;- EvaluationMode(defineEvaluationModes()[3]) # convenient definitions, I will reuse in the coming book chapters to simplify writing emo &lt;- list( standard = EvaluationMode(defineEvaluationModes()[1]), enhanced = EvaluationMode(defineEvaluationModes()[2]), type = EvaluationMode(defineEvaluationModes()[3]) ) "],
["running-functions.html", " 7 Running functions 7.1 Prerequisites 7.2 Transient invocations 7.3 Persistent invocations", " 7 Running functions Our goal is to run a R instrumented function, under a given evaluation mode. To do so, some prerequisites have to be met, prior to use some wyz.code.offensiveProgramming utilities to proceed to the function execution with context capture and human-readable feedback generation. 7.1 Prerequisites To run a R function, it has to be instrumented. Two requirements have to be met the function must comply with semantic parameter naming the function return type must be specified These prerequesites might be matched in two ways depending of transient or persistent invocations. Persistent invocation means that function return type information is stored into an object according to a convention, and therefore could be retrieved from it to be used and reused whenever and wherever needed. 7.2 Transient invocations Transient invocation means that function return type information is is explicitely set/given at invocation time. Invocation will be achieved dynamically and not persisted anywhere. In particular invocation context remains volatile, as long as you do not save it into a piece of code. Transient invocation is a convenient way to discover and to play with an offensive programming instrumented package. 7.2.1 Nominal case Here is a typical to invoque an offensive programming instrumented function in a transient way. Use function runTransientFunction to achieve desired result. To get the definition for ‘emo’ variable, please refer to 6. h &lt;- function(x_s) x_s runTransientFunction(h, list(&#39;neonira&#39;), emo$type, &#39;x_s&#39;) #&gt; $status #&gt; [1] TRUE #&gt; #&gt; $value #&gt; [1] &quot;neonira&quot; #&gt; #&gt; $mode #&gt; [1] &quot;type_checking_enforcement&quot; #&gt; #&gt; $parameter_type_checks #&gt; parameter_name parameter_value validity message #&gt; 1: x_s neonira TRUE good type in values #&gt; #&gt; $function_return_type_check #&gt; parameter_name parameter_value validity message #&gt; 1: x_s neonira TRUE good type in values Semantically, function h takes a vector of strings as argument and returns a vector of strings. As provided parameter ‘neonira’ is a vector of type character, parameter_type_checks succeed. As returned value is a vector of type character, function_return_type_check also succeeds. Therefore global status is TRUE. Generally, when used to use this function, you just jump onto the status and go deeper only when negative. 7.2.1.1 Wrong parameter type Let’s change the provided argument from ‘neonira’ to pi value. h &lt;- function(x_s) x_s runTransientFunction(h, list(pi), emo$type, &#39;x_s&#39;) #&gt; $status #&gt; [1] FALSE #&gt; #&gt; $value #&gt; [1] 3.141593 #&gt; #&gt; $mode #&gt; [1] &quot;type_checking_enforcement&quot; #&gt; #&gt; $parameter_type_checks #&gt; parameter_name parameter_value validity message #&gt; 1: x_s 3.141593 FALSE wrong type in values #&gt; #&gt; $function_return_type_check #&gt; parameter_name parameter_value validity message #&gt; 1: x_s 3.141593 FALSE wrong type in values The status is FALSE. Looking at details, we see that root causes are parameter_type_checks failure and function_return_type_check failure. We faced a case where function specification is unchanged but provided argument is not complying to specification, thus generating a double error. If you have a great experience of R, you know that discovering such root causes are challenging due to weakly typed and script nature of the R language. 7.2.1.2 Change expected return type Let’s change the expected function return type, to be double x_d. h &lt;- function(x_s) x_s runTransientFunction(h, list(pi), emo$type, &#39;x_d&#39;) #&gt; $status #&gt; [1] FALSE #&gt; #&gt; $value #&gt; [1] 3.141593 #&gt; #&gt; $mode #&gt; [1] &quot;type_checking_enforcement&quot; #&gt; #&gt; $parameter_type_checks #&gt; parameter_name parameter_value validity message #&gt; 1: x_s 3.141593 FALSE wrong type in values #&gt; #&gt; $function_return_type_check #&gt; parameter_name parameter_value validity message #&gt; 1: x_d 3.141593 TRUE good type in values The status is still FALSE but there is now one single root cause that is parameter_type_checks failure. We now face a case where function specification is unchanged but expected return type is expected to be a double. Provided argument is not complying to specification. To fix the issue, just rename the parameter to be x_d or any other defined and recorded factory type that matches a double. 7.2.2 Prerequisite mismatch What if function is not fulfilling prerequisites g &lt;- function(x) x # No semantic name compliance runTransientFunction(g, list(pi), EvaluationMode(defineEvaluationModes()[3]), &#39;x_d&#39;) #&gt; $status #&gt; [1] FALSE #&gt; #&gt; $value #&gt; [1] 3.141593 #&gt; #&gt; $mode #&gt; [1] &quot;type_checking_enforcement&quot; #&gt; #&gt; $parameter_type_checks #&gt; parameter_name parameter_value validity message #&gt; 1: x 3.141593 FALSE unknown suffix, [NA] #&gt; #&gt; $function_return_type_check #&gt; parameter_name parameter_value validity message #&gt; 1: x_d 3.141593 TRUE good type in values Impact is parameter_type_checks failure. This reminds you to verify name compliance cf. 5.2 before testing transient invocations. 7.2.3 Object function call What if the function you desire to call is an object function? In such a case, be sure to pass the object as first parameter of the list of parameters, as shown by example below, based on a S3 object. A priori, all kind of R objects are supported: S3, S4, RC, R6 and environment objects. 7.2.3.1 Right way library(data.table) source(file.path(system.file(package = &#39;wyz.code.offensiveProgramming&#39;), &#39;code-samples&#39;, &#39;both-defs/good/partial&#39;, &#39;Addition_TCFI_Partial_S3.R&#39;), encoding = &#39;UTF-8&#39;) a &lt;- Addition_TCFI_Partial_S3() runTransientFunction(addInteger.Addition_TCFI_Partial_S3, list(a, 3L, 4L), emo$type, &#39;x_i&#39;) #&gt; $status #&gt; [1] FALSE #&gt; #&gt; $value #&gt; [1] 7 #&gt; #&gt; $mode #&gt; [1] &quot;type_checking_enforcement&quot; #&gt; #&gt; $parameter_type_checks #&gt; parameter_name parameter_value validity #&gt; 1: object_o_1 &lt;Addition_TCFI_Partial_S3&gt; FALSE #&gt; 2: x_i 3 TRUE #&gt; 3: y_i 4 TRUE #&gt; message #&gt; 1: wrong length, was expecting [1] , got [3] #&gt; 2: good type in values #&gt; 3: good type in values #&gt; #&gt; $function_return_type_check #&gt; parameter_name parameter_value validity message #&gt; 1: x_i 7 TRUE good type in values 7.2.3.2 Omitting object What if I omit object as first parameter? runTransientFunction(addInteger.Addition_TCFI_Partial_S3, list(NULL, 3L, 4L), emo$type, &#39;x_i&#39;) #&gt; $status #&gt; [1] FALSE #&gt; #&gt; $value #&gt; [1] 7 #&gt; #&gt; $mode #&gt; [1] &quot;type_checking_enforcement&quot; #&gt; #&gt; $parameter_type_checks #&gt; parameter_name parameter_value validity #&gt; 1: object_o_1 FALSE #&gt; 2: x_i 3 TRUE #&gt; 3: y_i 4 TRUE #&gt; message #&gt; 1: wrong length, was expecting [1] , got [0] #&gt; 2: good type in values #&gt; 3: good type in values #&gt; #&gt; $function_return_type_check #&gt; parameter_name parameter_value validity message #&gt; 1: x_i 7 TRUE good type in values 7.3 Persistent invocations Transient invocations are convenient but limited. Especially, when you create classes, they do not appear to be as friendly and useful as necessary. When dealing with your own class code, you may opt for an easier and more industrial approach that is class instrumentation. Prerequisite remains the same, but you may fulfill them much more easily by defining a variable named function_return_type in your class. 7.3.1 Verifying recorded information To verify function return types definition of a R object, you may use the low level function verifyFunctionReturnTypesDefinition or the higher level one named retrieveFunctionReturnTypes. retrieveFunctionReturnTypes(AdditionTCFIG1()) #&gt; function_name return_value #&gt; 1: addDouble x_d #&gt; 2: addInteger x_i #&gt; 3: addMultiDouble x_d #&gt; 4: divideByZero x_d #&gt; 5: addMultiInteger x_i #&gt; 6: generateWarning x_w #&gt; 7: generateError x_er 7.3.2 Nominal persistent case Here is a typical to invoque an offensive programming instrumented function in a peristent way. Use function runFunction to achieve desired result. source(system.file(&#39;code-samples/frt-defs/good/partial/AdditionFIPartial.R&#39;, package = &#39;wyz.code.offensiveProgramming&#39;)) runFunction(AdditionFIPartial(), &#39;addInteger&#39;, list(1:3, 6:8), emo$type) #&gt; $status #&gt; [1] TRUE #&gt; #&gt; $value #&gt; [1] 7 9 11 #&gt; #&gt; $mode #&gt; [1] &quot;type_checking_enforcement&quot; #&gt; #&gt; $function_return_check #&gt; [1] TRUE #&gt; #&gt; $parameter_check #&gt; [1] TRUE #&gt; #&gt; $parameter_type_checks #&gt; parameter_name parameter_value validity message #&gt; 1: x_i 1,2,3 TRUE good type in values #&gt; 2: y_i 6,7,8 TRUE good type in values #&gt; #&gt; $function_return_type_check #&gt; parameter_name parameter_value validity message #&gt; 1: x_i 7, 9,11 TRUE good type in values The status is TRUE expressing parameter_type_checks and function_return_type_check compliance. 7.3.3 Subtile change Just change the function name. Now expectations brought by the function definition are not the same. In particular semantic names for function parameters and function return types are no more correct. runFunction(AdditionFIPartial(), &#39;addDouble&#39;, list(1:3, 6:8), emo$type) #&gt; $status #&gt; [1] FALSE #&gt; #&gt; $value #&gt; [1] 7 9 11 #&gt; #&gt; $mode #&gt; [1] &quot;type_checking_enforcement&quot; #&gt; #&gt; $function_return_check #&gt; [1] FALSE #&gt; #&gt; $parameter_check #&gt; [1] FALSE #&gt; #&gt; $parameter_type_checks #&gt; parameter_name parameter_value validity message #&gt; 1: x_d 1,2,3 FALSE wrong type in values #&gt; 2: y_d 6,7,8 FALSE wrong type in values #&gt; #&gt; $function_return_type_check #&gt; parameter_name parameter_value validity message #&gt; 1: x_d 7, 9,11 FALSE wrong type in values The status is FALSE expressing the two incompliances. 7.3.4 Call case with named and positional parameters runFunction(AdditionFIPartial(), &#39;addInteger&#39;, list(y_i = 1:3, 6:8), emo$type) #&gt; $status #&gt; [1] TRUE #&gt; #&gt; $value #&gt; [1] 7 9 11 #&gt; #&gt; $mode #&gt; [1] &quot;type_checking_enforcement&quot; #&gt; #&gt; $function_return_check #&gt; [1] TRUE #&gt; #&gt; $parameter_check #&gt; [1] TRUE #&gt; #&gt; $parameter_type_checks #&gt; parameter_name parameter_value validity message #&gt; 1: y_i 1,2,3 TRUE good type in values #&gt; 2: x_i 6,7,8 TRUE good type in values #&gt; #&gt; $function_return_type_check #&gt; parameter_name parameter_value validity message #&gt; 1: x_i 7, 9,11 TRUE good type in values Look at parameter_type_checks to ensure arguments are well associated. 7.3.5 Call case with ellipsis runFunction(AdditionFIPartial(), &#39;addMultiDouble&#39;, list(1:3, 1:7), emo$type) #&gt; $status #&gt; [1] TRUE #&gt; #&gt; $value #&gt; [1] 34 #&gt; #&gt; $mode #&gt; [1] &quot;type_checking_enforcement&quot; #&gt; #&gt; $function_return_check #&gt; [1] TRUE #&gt; #&gt; $parameter_check #&gt; [1] TRUE #&gt; #&gt; $parameter_type_checks #&gt; parameter_name parameter_value validity message #&gt; 1: ... 1,2,3 TRUE ellipsis matches all #&gt; 2: ... 1,2,3,4,5,6,... TRUE ellipsis matches all #&gt; #&gt; $function_return_type_check #&gt; parameter_name parameter_value validity message #&gt; 1: x_d 34 TRUE good type in values As ellipsis matches all inputs, this case behaves correctly. 7.3.6 Second call case with ellipsis runFunction(AdditionFIPartial(), &#39;addMultiInteger&#39;, list(1:3, 1:7, 0, floor(pi)), emo$type) #&gt; $status #&gt; [1] FALSE #&gt; #&gt; $value #&gt; [1] 32 33 34 #&gt; #&gt; $mode #&gt; [1] &quot;type_checking_enforcement&quot; #&gt; #&gt; $function_return_check #&gt; [1] FALSE #&gt; #&gt; $parameter_check #&gt; [1] TRUE #&gt; #&gt; $parameter_type_checks #&gt; parameter_name parameter_value validity message #&gt; 1: x_i 1,2,3 TRUE good type in values #&gt; 2: ... 1,2,3,4,5,6,... TRUE ellipsis matches all #&gt; 3: ... 0 TRUE ellipsis matches all #&gt; 4: ... 3 TRUE ellipsis matches all #&gt; #&gt; $function_return_type_check #&gt; parameter_name parameter_value validity message #&gt; 1: x_i 32,33,34 FALSE wrong type in values Here, ellipsis still matches all, but parameter list is type heterogeneous. This brings two issues. Value 0 is double, not integer and floor function returns also a double. To get a correct results, enforce inout parameter coercision to expected type. runFunction(AdditionFIPartial(), &#39;addMultiInteger&#39;, list(1:3, 1:7, 0L, as.integer(floor(pi))), emo$type) #&gt; $status #&gt; [1] TRUE #&gt; #&gt; $value #&gt; [1] 32 33 34 #&gt; #&gt; $mode #&gt; [1] &quot;type_checking_enforcement&quot; #&gt; #&gt; $function_return_check #&gt; [1] TRUE #&gt; #&gt; $parameter_check #&gt; [1] TRUE #&gt; #&gt; $parameter_type_checks #&gt; parameter_name parameter_value validity message #&gt; 1: x_i 1,2,3 TRUE good type in values #&gt; 2: ... 1,2,3,4,5,6,... TRUE ellipsis matches all #&gt; 3: ... 0 TRUE ellipsis matches all #&gt; 4: ... 3 TRUE ellipsis matches all #&gt; #&gt; $function_return_type_check #&gt; parameter_name parameter_value validity message #&gt; 1: x_i 32,33,34 TRUE good type in values "],
["running-test-cases.html", " 8 Running test cases 8.1 Embedding test cases 8.2 Discovering test cases 8.3 Running test cases", " 8 Running test cases Using package wyz.code.offensiveProgramming, you have the opportunity to define test cases and to embed them in your class definition, to ease retrieval and reuse. Doing so, allows to get following benefits discover defined test case definitions run of any test case definition get interactively the R code of a test case, allowing you to play with it, manually, when needed get contextual results from the test case runs 8.1 Embedding test cases Embedding test cases in class definition is accomplished by declaring a variable named testCaseDefinitions, and providing its content, that is a data.table. Content could be partial or complete depending of your goals. Spectrum of provided tests cases is as you desire it to be, as shallow or deep as needed. The data.table must hold following columns and content function_name, a vector of strings, each being the name of the function to test, standard_evaluation, a vector of strings, where values are taken from set {‘correct’, ‘erroneous’, ‘failure’} type_checking_enforcement, a vector of strings, where values are taken from set {‘correct’, ‘erroneous’, ‘failure’} test case definitions, that is a list of TestCaseDefinition objects. Correct implies right type and right result. Erroneous implies right type and wrong result. Failure implies wrong type. To get more details about syntax, please refer to manual page of TestCaseDefinition. 8.2 Discovering test cases The considered R object has to be instrumented with test case definitions. 8.2.1 Test case verifications To verify test cases definitions use function verifyTestCaseDefinitions. verifyTestCaseDefinitions(AdditionTCFIG1()) #&gt; $validity #&gt; [1] TRUE #&gt; #&gt; $check #&gt; [1] &quot;full instrumentation check&quot; #&gt; #&gt; $class #&gt; [1] &quot;AdditionTCFIG1&quot; #&gt; #&gt; $intent #&gt; [1] &quot;naming and instrumentation format and content seems good&quot; #&gt; #&gt; $message #&gt; [1] &quot;success&quot; Here, the validity flag is TRUE and checks have been done against full instrumentation, meaning each function of the object must own at least one associated test case. If this does not match your need, specify requiresFullInstrumentation_b_1 = FALSE as parameter to the call. 8.2.2 Getting instrumented functions Use function retrieveTestCaseDefinitions to get access to recorded test case definitions tc &lt;- retrieveTestCaseDefinitions(AdditionTCFIG1()) if (is.data.table(tc)) { tc[, .N, by = &#39;function_name&#39;] } #&gt; function_name N #&gt; 1: addDouble 9 #&gt; 2: addInteger 9 #&gt; 3: divideByZero 3 #&gt; 4: generateWarning 1 #&gt; 5: generateError 1 #&gt; 6: addMultiDouble 3 #&gt; 7: addMultiInteger 3 8.2.2.1 No test cases instrumentation retrieveTestCaseDescriptions(Addition()) #&gt; [1] &quot;provided object owns no test case definitions&quot; 8.2.3 Getting test cases descriptions Use function retrieveTestCaseDescriptions to get access to recorded test case descriptions. tc &lt;- retrieveTestCaseDescriptions(AdditionTCFIG1()) if (is.data.table(tc)) { cat(nrow(tc), &#39;test cases\\n&#39;) tc[, .N, by = &#39;function_name&#39;] } #&gt; 29 test cases #&gt; function_name N #&gt; 1: addDouble 9 #&gt; 2: addInteger 9 #&gt; 3: divideByZero 3 #&gt; 4: generateWarning 1 #&gt; 5: generateError 1 #&gt; 6: addMultiDouble 3 #&gt; 7: addMultiInteger 3 Getting test case description for function divideByZero, can be achieved with a sequence like tc[function_name == &#39;divideByZero&#39;] #&gt; function_name description #&gt; 1: divideByZero 1 / 0 #&gt; 2: divideByZero -1 / 0 #&gt; 3: divideByZero 0 / 0 Of course, no obligation to restrict output to a single function as in this example. Do not be fooled, description is text not code as you could deduce from previous example. See below. tc[function_name == &#39;generateWarning&#39;] #&gt; function_name description #&gt; 1: generateWarning generate warning 8.3 Running test cases 8.3.1 Run a single test case I want to run the test number 4 of object AdditionTCFIG1. Let’s first, know more about it. test_to_run &lt;- 4 obj &lt;- AdditionTCFIG1() # instanciate your object tc &lt;- retrieveTestCaseDefinitions(obj)[test_to_run] print(tc) #&gt; function_name standard_evaluation type_checking_enforcement #&gt; 1: addDouble correct correct #&gt; test_case #&gt; 1: &lt;TestCaseDefinition&gt; tc[1]$test_case[[1]] #&gt; $params #&gt; $params[[1]] #&gt; [1] NaN #&gt; #&gt; $params[[2]] #&gt; [1] NaN #&gt; #&gt; #&gt; $expected_result #&gt; [1] NaN #&gt; #&gt; $description #&gt; [1] &quot;sum 2 NAN&quot; To run the chosen test case, use function runTestCase. rtc &lt;- runTestCase(obj, test_to_run, emo$type) rtc$synthesis #&gt; status mode index value_check function_return_check #&gt; 1: TRUE type_checking_enforcement 4 TRUE TRUE #&gt; parameter_check expected_evaluation execution_evaluation failure_origin #&gt; 1: TRUE correct correct &lt;NA&gt; Result has two parts. A raw part, that holds the intermediate computation results, and a synthesis part, that is the only one shown above. Here, the status flag is TRUE meaning test case behaves perfectly in accordance with declared semantic names, under provided values. 8.3.2 Run several test cases You can provide a vector instead of a single test number if you want to run several use test cases in one call. test_to_run &lt;- 12:14 tc &lt;- retrieveTestCaseDefinitions(obj)[test_to_run] print(tc) #&gt; function_name standard_evaluation type_checking_enforcement #&gt; 1: addInteger correct correct #&gt; 2: addInteger correct correct #&gt; 3: addInteger erroneous failure #&gt; test_case #&gt; 1: &lt;TestCaseDefinition&gt; #&gt; 2: &lt;TestCaseDefinition&gt; #&gt; 3: &lt;TestCaseDefinition&gt; lapply(seq_len(nrow(tc)), function(k) print(tc[k]$test_case[[1]])) #&gt; $params #&gt; $params[[1]] #&gt; [1] NA #&gt; #&gt; $params[[2]] #&gt; [1] NA #&gt; #&gt; #&gt; $expected_result #&gt; [1] NA #&gt; #&gt; $description #&gt; [1] &quot;sum 2 NA_integer&quot; #&gt; #&gt; $params #&gt; $params[[1]] #&gt; [1] 45 #&gt; #&gt; $params[[2]] #&gt; [1] 44 #&gt; #&gt; #&gt; $expected_result #&gt; [1] 89 #&gt; #&gt; $description #&gt; [1] &quot;sum a converted string with one integer&quot; #&gt; #&gt; $params #&gt; $params[[1]] #&gt; [1] 34 #&gt; #&gt; $params[[2]] #&gt; [1] 44.5 #&gt; #&gt; #&gt; $expected_result #&gt; [1] 78 #&gt; #&gt; $description #&gt; [1] &quot;sum 1 integer and 1 double&quot; #&gt; [[1]] #&gt; [[1]]$params #&gt; [[1]]$params[[1]] #&gt; [1] NA #&gt; #&gt; [[1]]$params[[2]] #&gt; [1] NA #&gt; #&gt; #&gt; [[1]]$expected_result #&gt; [1] NA #&gt; #&gt; [[1]]$description #&gt; [1] &quot;sum 2 NA_integer&quot; #&gt; #&gt; #&gt; [[2]] #&gt; [[2]]$params #&gt; [[2]]$params[[1]] #&gt; [1] 45 #&gt; #&gt; [[2]]$params[[2]] #&gt; [1] 44 #&gt; #&gt; #&gt; [[2]]$expected_result #&gt; [1] 89 #&gt; #&gt; [[2]]$description #&gt; [1] &quot;sum a converted string with one integer&quot; #&gt; #&gt; #&gt; [[3]] #&gt; [[3]]$params #&gt; [[3]]$params[[1]] #&gt; [1] 34 #&gt; #&gt; [[3]]$params[[2]] #&gt; [1] 44.5 #&gt; #&gt; #&gt; [[3]]$expected_result #&gt; [1] 78 #&gt; #&gt; [[3]]$description #&gt; [1] &quot;sum 1 integer and 1 double&quot; rtc &lt;- runTestCase(obj, test_to_run, emo$type) rtc$synthesis #&gt; status mode index value_check function_return_check #&gt; 1: TRUE type_checking_enforcement 12 TRUE TRUE #&gt; 2: TRUE type_checking_enforcement 13 TRUE TRUE #&gt; 3: FALSE type_checking_enforcement 14 FALSE FALSE #&gt; parameter_check expected_evaluation execution_evaluation #&gt; 1: TRUE correct correct #&gt; 2: TRUE correct correct #&gt; 3: FALSE failure failure #&gt; failure_origin #&gt; 1: &lt;NA&gt; #&gt; 2: &lt;NA&gt; #&gt; 3: value check, function return type check Test 14 fails under chosen evaluation mode, and therefore requires a fix. Here looking at raw results will provide root causes of failure. Provided values are double, whereas integers were expected. To understand the full details, you will have to run this example and to print rtc globally to get access to many details far too long to be displayed here. "],
["generating-testthat-test-files.html", " 9 Generating testthat test files 9.1 Package wyz.code.testthat in action 9.2 Generated unit test file content 9.3 Caveats", " 9 Generating testthat test files When your R code is offensive programming instrumented, it becomes possible to generate testthat unit test files, thus improving greatly developers productivity. To be able to generate testthat unit test files, you must ensure that your R code is function return type instrumented and test cases instrumented. Both are required for this generation. 9.1 Package wyz.code.testthat in action 9.1.1 Setting up the context To create testthat unit test files, you must provide a target folder, to store the generated unit test files. In this session, generated files will be stored onto folder generated-testthat. You may change this, but be warned that generated files might be overwritten without any reminder. Be careful, if you set the target folder to tests/testthat as you may loose previous work. You may save frequently your results in configuration management to be able to retrieve original file content whenever required. library(wyz.code.testthat) target_folder &lt;- &#39;generated-testthat&#39; if (!dir.exists(target_folder)) dir.create(target_folder) generateTests &lt;- function(sourceFile_s_1, sourcePackage_s_1, object_o_1) { g &lt;- gautfo(object_o_1, sourceFile_s_1, sourcePackage_s_1, target_folder) print(g) g } Function generateTests is just a wrapper which prints the computed results. 9.1.2 Loading objects Here, I reuse 4 files from package wyz.code.offensiveProgramming to generate unit test files from. source_package &lt;- &#39;wyz.code.offensiveProgramming&#39; source_files &lt;- c( &#39;code-samples/both-defs/good/full/AdditionTCFIG1.R&#39;, &#39;code-samples/no-defs/Addition.R&#39;, &#39;code-samples/frt-defs/good/partial/AdditionFIPartial.R&#39;, &#39;code-samples/tcd-defs/good/partial/AdditionTCPartial.R&#39; ) invisible(sapply(source_files, function(e) { source(system.file(e, package = source_package)) })) Objects are chosen to cover some very common cases and to show variability of results when generating testthat test cases from. object information AdditionTCFIG1 Fully offensive programming instrumented Addition no offensive programming instrumentation AdditionFIPartial.R partial function return type definition, no test case definitions AdditionTCPartial.R no function return type definition, partial test case definitions 9.1.3 Unit test file generation Generation is done on a per class basis and is achieved using function gautfo or function generateAllUnitTestsFromObject. 9.1.3.1 Nominal case Class AdditionTCFIG1 is used. Generation is quite straightforward. print(gautfo(AdditionTCFIG1(), source_files[1], source_package, target_folder)) #&gt; $class #&gt; [1] &quot;AdditionTCFIG1&quot; #&gt; #&gt; $filenames #&gt; filename overwritten #&gt; 1: generated-testthat/test_AdditionTCFIG1-addDouble.R TRUE #&gt; 2: generated-testthat/test_AdditionTCFIG1-addInteger.R TRUE #&gt; 3: generated-testthat/test_AdditionTCFIG1-divideByZero.R TRUE #&gt; 4: generated-testthat/test_AdditionTCFIG1-generateWarning.R TRUE #&gt; 5: generated-testthat/test_AdditionTCFIG1-generateError.R TRUE #&gt; 6: generated-testthat/test_AdditionTCFIG1-addMultiDouble.R TRUE #&gt; 7: generated-testthat/test_AdditionTCFIG1-addMultiInteger.R TRUE A file is created for each function for which test case definitions are defined. This file contains all the generated tests for two evaluation modes that are standard_R_evaluation and type_checking_enforcement. Each mode get its own testthat context. Results is a list with two entries. Entry named filenames holds a data.table providing insight about testthat compliant created files. 9.1.3.2 Exception cases All other objects do not reach miminum level of offensive programming instrumentation to allow test generation. Expected results is no testthat unit test file generation. print(gautfo(Addition(), source_files[2], source_package, target_folder)) #&gt; [1] &quot;Class [Addition] apparently owns no test instrumentation. No test created.&quot; print(gautfo(AdditionFIPartial(), source_files[3], source_package, target_folder)) #&gt; [1] &quot;Class [AdditionFIPartial] apparently owns no test instrumentation. No test created.&quot; print(gautfo(AdditionTCPartial(), source_files[4], source_package, target_folder)) #&gt; [1] &quot;Class [AdditionTCPartial] apparently owns no function return type instrumentation. No test created.&quot; 9.2 Generated unit test file content Typically, generated R code will looks like following unit test code. Note, that comments are provided to ease cross-referencing and to link back easily to wyz.code.offensiveProgramming test case number. This is helpful when facing dysfunctions. source(system.file(&quot;code-samples/both-defs/good/full/AdditionTCFIG1.R&quot;, package = &quot;wyz.code.offensiveProgramming&quot;)) object_o_1 &lt;- AdditionTCFIG1() emsre &lt;- EvaluationMode(&quot;standard_R_evaluation&quot;) rtcsre24 &lt;- runTestCase(object_o_1, 24, emsre) rtcsre25 &lt;- runTestCase(object_o_1, 25, emsre) rtcsre26 &lt;- runTestCase(object_o_1, 26, emsre) test_that(&#39;addMultiDouble&#39;, { # test 24 - sum of 1 integer and 1 double - correct expect_true(rtcsre24$synthesis$status) expect_true(rtcsre24$synthesis$value_check) # test 25 - sum of 1 double, 2 integers and 1 NA_integer_ - correct expect_true(rtcsre25$synthesis$status) expect_true(rtcsre25$synthesis$value_check) # test 26 - sum of nothing - correct expect_true(rtcsre26$synthesis$status) expect_true(rtcsre26$synthesis$value_check) }) emtce &lt;- EvaluationMode(&quot;type_checking_enforcement&quot;) rtctce24 &lt;- runTestCase(object_o_1, 24, emtce) rtctce25 &lt;- runTestCase(object_o_1, 25, emtce) rtctce26 &lt;- runTestCase(object_o_1, 26, emtce) test_that(&#39;addMultiDouble&#39;, { # test 24 - sum of 1 integer and 1 double - correct expect_true(rtctce24$synthesis$status) expect_true(rtctce24$synthesis$value_check) # test 25 - sum of 1 double, 2 integers and 1 NA_integer_ - correct expect_true(rtctce25$synthesis$status) expect_true(rtctce25$synthesis$value_check) # test 26 - sum of nothing - correct expect_true(rtctce26$synthesis$status) expect_true(rtctce26$synthesis$value_check) }) 9.3 Caveats Generation of unit test file uses meta-programmation based on call function, and aims to produce R valid code. Indeed, format and presentation are not managed, in generated file. You may use RStudio editing facilities to ensure nice presentation, although neither mandatory nor required. Generated tests cases are ready to run. Use the standard way to run your testthat test cases onto them. If you face some test failures, verify following points. ensure offensive programming evaluation of your code is running fine for ALL evaluation schemes, and that there exists no residual issues prior to generate test files ensure your generated test files are well up to date with the your R offensive programming code. You may regenerate your tests files at any time if you have any doubt. Note that unit test file generated is fully dependent of your R source and of the instrumented scope. If there are function not instrumented in your source code, do not expect to have unit test cases for them. If you plan to use (ref:tt), then go for fully instrumented offensive programming classes. This will help you achieve results faster. One severe issue of partial compliance, is that you have to remember all the exceptions. When dealing with few classes, quite easy, when dealing with several classes, it becomes messy to keep a clear view of what is instrumented or not. Not because of lack of tools, simply because you cannot anymore keep it present in your mind. Also note, when working incrementally, i.e. creating some code, generating related testthat test files, and iterating the procedure, you need to regenerate the unit test cases each time you change the R source code or the offensive programming instrumentation at the scope of changes. If you change 2 classes, regenerate the test files of the 2 classes. Again, keeping the books of such approch is heavy and cumbersome. You would better go for systematic and global testthat test file generation, except if you modified manually produced results. Best way to put wyz.code.testthat in practice, is to apply following procedure apply offensive programming at the required scope and ensure wyz.code.offensiveProgramming test cases are valid, using runTestCase function generate testthat test cases in one single pass using wyz.code.testthat. To do so, create an R script. This will ease your pain, and will provide consistent results through calls while allowing replay at any time, apply testthat testing practice, to verify that generated tests are running fine. "],
["meta-testing.html", " 10 Meta-testing 10.1 Traditional way of testing 10.2 Meta testing 10.3 A more complex example 10.4 An example using ellipsis 10.5 A tricky example 10.6 Pitfalls to avoid", " 10 Meta-testing Meta-testing is the activity aiming to test a function while providing no data to test it. In a R context, it means being able to discover function signature infer data type for each argument generate data set to be used for each argument run the function with generated data sets give back some summary statistics about discoveries of various test run achieved If you try it by hand, you will probably succeed, because the second point will be managed directly by your brain. If you try it by a program, type inference is much trickier, because any argument in R could be of any type. Generally, you need the documentation and explanations to restrict the scope of possible types. That’s where using wyz.code.meta-testing will ease your work and bring instrumentation to get results in a more reliable and quicker way. 10.1 Traditional way of testing By testing, I mean hand testing and operating scope and limits discovery. 10.1.1 Empirical approach If you opt for an empirial testing of function cos, then you will enter command lines for each case you want to test. If you need many of them, it could become really boring very fast. It also disturbates you from the analysis of the results. Moreover, in this approach you are nearly forced to open the black box cos and to get knowledge about it. That’s where it hurts, as working under predictable time in those conditions is really difficult. 10.1.2 More industrial approach Going a little bit further than previous, you just use loops and scripts to ease replay. Let’s test base function cos in this way. rt &lt;- tryCatch( lapply(list(2 * pi / 1:9, NA, Inf, 1+1i, list(), letters[1:3]), cos), error = function(e) { print(e); NaN } ) #&gt; Warning in FUN(X[[i]], ...): NaNs produced #&gt; &lt;simpleError in FUN(X[[i]], ...): non-numeric argument to mathematical function&gt; And now, we have to unravel the arguments to find which ones are generating warnings or errors, to identify the ones that are accepted. Note that this trial is a gentle trial. It does not try for example to provide raw type or data frame or matrix or a function as argument. Again, you will have to get knowledge about the function to be able to provide expected parameter types and values. You also would have to decipher the R documentation related to the function. Some documentations are easy and clear, some others much more difficult to understand and interpret. 10.2 Meta testing 10.2.1 Meta testing approach Let’s test function op_cos using wyz.code.meta-testing. The general procedure to follow is create a wrapper function if your function is not offensive programming instrumented get some knowlegde about the function signature complexity, fire massive tests, and get synthesis results 10.2.2 Create a wrapper function Any R function can be classified as offensive programming compliant or not. Second case is indeed much more common and will be encountered more often. In such a case, use offensiveProgrammingWrapFunction or function opwf to generate a new R function that will be offensive programming compliant. You have to provide semantic argument names to this function to be able to generate correctly the wrapping function. Once done, type inference is now driven by semantic argument names. Let’s see an example, considering function cos from base package. library(wyz.code.metaTesting) op_cos &lt;- opwf(cos, &#39;radianAngleOrComplex_&#39;) op_cos #&gt; function (radianAngleOrComplex_) #&gt; { #&gt; cos(radianAngleOrComplex_) #&gt; } #&gt; &lt;environment: 0x557d990aaca0&gt; You may wonder what are the difference between the standard R signature and this one? They share same number of arguments, just the name changed. Yes, but as the name is now a semantic name, it can be managed by a factory See FunctionParameterTypeFactory in wyz.code.offensiveProgramming for more details to generate data to match the parameters. 10.2.3 Getting some complexity knowledge This is achieved using function computeArgumentsCombination. rv &lt;- computeArgumentsCombination(op_cos) rv$theoritical_signature_number #&gt; [1] 1 Here there exist only one signature for function op_cos and so for function cos. Note that function computeArgumentsCombination can be used with any R function. It does not requires offensive programming instrumentated function as argument. 10.2.4 Firing massive tests This is achieved using function exploreSignatures. The second argument passed to the function is just a type restriction to be enforced when generating data for testing purposes. Here I asked for integer real and complex mathematical types these are different from R integer, double and complex as they cannot take value NA. Type restrictions are only considered for polymorphic arguments reminder: the ones that ends with an underscore. es &lt;- exploreSignatures(op_cos, list(radianAngleOrComplex_ = c(&#39;im&#39;, &#39;r&#39;, &#39;cm&#39;))) nm &lt;- names(es$success$synthesis) sx &lt;- nm[startsWith(nm, &#39;number_suc&#39;)][1] # typo error number_sucessful corrected in v1.1.12 of metaTesting cat(&#39;succesful tests:&#39;, es$success$synthesis[[sx]], &#39;\\n&#39;) #&gt; succesful tests: 6 cat(&#39;erroneous tests:&#39;, es$failure$synthesis$number_erroneous_tests, &#39;\\n&#39;) #&gt; erroneous tests: 6 Results tell you that same signature brings various results. Here, 6 tests succeeded and 6 failed. 10.2.4.1 Result analysis cat(&#39;error message:&#39;, es$failure$synthesis$error, &#39;\\n&#39;) #&gt; error message: Error in cos(radianAngleOrComplex_): non-numeric argument to mathematical function #&gt; All execution errors provided the same error message. First diagnostic result, errors are tied to usage of non numeric values. cat(&#39;succesful pattern:&#39;, es$success$synthesis$imperative, &#39;\\n&#39;) #&gt; succesful pattern: {homo,hetero}_{vector}_{one,two,three} cat(&#39;erroneous pattern:&#39;, es$failure$synthesis$imperative, &#39;\\n&#39;) #&gt; erroneous pattern: {homo,hetero}_{list}_{one,two,three} It is clear that the issue is tied to using a list as value to an imperative argument. Looking closer, on success, you see that only vectors provided results. Secon diagnostic results, list should not be used as input parameter. Now, you can conclude that cos function accepts as input vectors of integers, reals and complex passing a list as argument brings an error with the shown message. To be complete, note that as I enforced mathematical arguments, values NA, NaN and Inf are no more possible values for test. This match the mathematic function cosinus and not the R function cos. This is an important point, know what scope you want to test, not just what function you want to test I was expecting the cosinus of a complex number to compute the cosinus of the argument of the complex number, normalized by its modulus. Was expecting a ℂ to ℝ function. That is clearly not the case as output are complex numbers. That is a ℂ to ℂ function. Explanation of the behavior is that the function cosine is extended to complex numbers as explained here. If you read R documentation of cos function, you would not get any relevant information about this. 10.3 A more complex example Let’s now use function append from base package. 10.3.1 Create a wrapper function As you know, we need first to create the offensive programming wrapper function. op_append &lt;- opwf(append, c(&#39;originalValues_&#39;, &#39;valuesToInsert_&#39;, &#39;afterIndex_ui_1&#39;)) op_append #&gt; function (originalValues_, valuesToInsert_, afterIndex_ui_1 = length(originalValues_)) #&gt; { #&gt; append(originalValues_, valuesToInsert_, after = afterIndex_ui_1) #&gt; } #&gt; &lt;environment: 0x557d97f161f8&gt; As you can see, parameter substitution is also achieved in code for default arguments. 10.3.2 Getting some complexity knowledge How complex is it to test this function? rv &lt;- computeArgumentsCombination(op_append) rv$theoritical_signature_number #&gt; [1] 2 rv$signatures #&gt; [[1]] #&gt; [1] &quot;originalValues_, valuesToInsert_&quot; #&gt; #&gt; [[2]] #&gt; [1] &quot;originalValues_, valuesToInsert_, afterIndex_ui_1&quot; There are two call signatures, one without default parameter, one with. Let’s test them. 10.3.3 Firing massive tests es &lt;- exploreSignatures(op_append) print(es$success$synthesis) #&gt; $number_successfull_tests #&gt; [1] 24 #&gt; #&gt; $signatures #&gt; [1] &quot;originalValues_, valuesToInsert_&quot; #&gt; [2] &quot;originalValues_, valuesToInsert_, afterIndex_ui_1&quot; #&gt; #&gt; $imperative #&gt; [1] &quot;{homo,hetero}_{vector,list}_{one,two,three}&quot; #&gt; #&gt; $default #&gt; [1] &quot;{none,full}&quot; print(es$failure$synthesis) #&gt; [1] NA 10.3.3.1 Result analysis From the 24 test runs, no errors where generated. 10.3.3.1.1 Instrospecting the results If you are curious about a particular test call, let’s say number 22, just introspect returned values as below. You will see the code use to call the function during the test. print(es$success$code[22]$call_string) #&gt; [1] &quot;op_append(originalValues_ = list(c(-12.9267629287206, 6.28514515282586, ---9.57221709983423, 5.77918772771955), c(0+0i, -9+15i, 16+13i, ---1-2i)), valuesToInsert_ = list(list(data = list(c(5.72568594152108, ----0.64659017464146, 0.983572392724454), c(0+9i, -2-3i, -1-1i, ----15+14i, 9-8i, 5-15i), 3L)), structure(c(22198, 12808, 12808), class = \\&quot;Date\\&quot;)), --- afterIndex_ui_1 = 7L)&quot; If you desire to introspect the call results, use this approach. print(es$success$code[22]$result) #&gt; [[1]] #&gt; [[1]][[1]] #&gt; [1] -12.926763 6.285145 9.572217 5.779188 #&gt; #&gt; [[1]][[2]] #&gt; [1] 0+ 0i -9+15i 16+13i 1- 2i #&gt; #&gt; [[1]][[3]] #&gt; [[1]][[3]]$data #&gt; [[1]][[3]]$data[[1]] #&gt; [1] 5.7256859 -0.6465902 0.9835724 #&gt; #&gt; [[1]][[3]]$data[[2]] #&gt; [1] 0+ 9i -2- 3i -1- 1i -15+14i 9- 8i 5-15i #&gt; #&gt; [[1]][[3]]$data[[3]] #&gt; [1] 3 #&gt; #&gt; #&gt; #&gt; [[1]][[4]] #&gt; [1] &quot;2030-10-11&quot; &quot;2005-01-25&quot; &quot;2005-01-25&quot; 10.4 An example using ellipsis Let’s now use function sum from base package. 10.4.1 Create a wrapper function As you know, we need first to create the offensive programming wrapper function. op_sum &lt;- opwf(sum, c(&#39;...&#39;, &#39;removeNAValues_b_1&#39;)) op_sum #&gt; function (..., removeNAValues_b_1 = FALSE) #&gt; { #&gt; sum(..., na.rm = removeNAValues_b_1) #&gt; } #&gt; &lt;environment: 0x557d9a545238&gt; As you can see, parameter substitution is also achieved in code for default arguments. 10.4.2 Getting some complexity knowledge How complex is it to test this function? cac_sum &lt;- computeArgumentsCombination(op_sum) print(cac_sum$signatures) #&gt; [[1]] #&gt; character(0) #&gt; #&gt; [[2]] #&gt; [1] &quot;removeNAValues_b_1&quot; #&gt; #&gt; [[3]] #&gt; [1] &quot;ellipsis1_&quot; #&gt; #&gt; [[4]] #&gt; [1] &quot;ellipsis1_, removeNAValues_b_1&quot; #&gt; #&gt; [[5]] #&gt; [1] &quot;ellipsis1_, ellipsis2_&quot; #&gt; #&gt; [[6]] #&gt; [1] &quot;ellipsis1_, ellipsis2_, removeNAValues_b_1&quot; #&gt; #&gt; [[7]] #&gt; [1] &quot;ellipsis1_, ellipsis2_, ellipsis3_&quot; #&gt; #&gt; [[8]] #&gt; [1] &quot;ellipsis1_, ellipsis2_, ellipsis3_, removeNAValues_b_1&quot; There are eight call signatures, four without default parameter, four with. By default, ellipsis is replaced by 0 to three arguments. That’s why, first signature is empty, and the total is 8. Let’s test them. 10.4.3 Firing massive tests es &lt;- exploreSignatures(op_sum) print(es$success$synthesis) #&gt; $number_successfull_tests #&gt; [1] 8 #&gt; #&gt; $signatures #&gt; [1] &quot;no argument signature&quot; &quot;removeNAValues_b_1&quot; #&gt; #&gt; $ellipsis #&gt; [1] &quot;{homo,hetero}_{vector,list}_{none}&quot; #&gt; #&gt; $default #&gt; [1] &quot;{none,full}&quot; print(es$failure$synthesis) #&gt; $number_erroneous_tests #&gt; [1] 24 #&gt; #&gt; $error #&gt; [1] &quot;Error in sum(..., na.rm = removeNAValues_b_1): invalid &#39;type&#39; (list) of argument\\n&quot; #&gt; #&gt; $signatures #&gt; [1] &quot;ellipsis1_&quot; #&gt; [2] &quot;ellipsis1_, removeNAValues_b_1&quot; #&gt; [3] &quot;ellipsis1_, ellipsis2_&quot; #&gt; [4] &quot;ellipsis1_, ellipsis2_, removeNAValues_b_1&quot; #&gt; [5] &quot;ellipsis1_, ellipsis2_, ellipsis3_&quot; #&gt; [6] &quot;ellipsis1_, ellipsis2_, ellipsis3_, removeNAValues_b_1&quot; #&gt; #&gt; $ellipsis #&gt; [1] &quot;{homo,hetero}_{vector,list}_{one,two,three}&quot; #&gt; #&gt; $default #&gt; [1] &quot;{none,full}&quot; From the 32 test runs, 8 passed, 24 failed. As we gave no restriction types for ellipsis, it has been replaced by any kind of types, and in particular some that cannot fit a sum. Let’s restrict the types to uses and run again same kind of test. es2 &lt;- exploreSignatures(op_sum, list(&#39;...&#39; = c(&#39;im&#39;, &#39;rm&#39;, &#39;cm&#39;))) Much better. Still 32 tests, now 20 passed, 12 failed. Why? 10.4.3.1 Result analysis print(es2$failure$synthesis) #&gt; $number_erroneous_tests #&gt; [1] 12 #&gt; #&gt; $error #&gt; [1] &quot;Error in sum(..., na.rm = removeNAValues_b_1): invalid &#39;type&#39; (list) of argument\\n&quot; #&gt; #&gt; $signatures #&gt; [1] &quot;ellipsis1_&quot; #&gt; [2] &quot;ellipsis1_, removeNAValues_b_1&quot; #&gt; [3] &quot;ellipsis1_, ellipsis2_&quot; #&gt; [4] &quot;ellipsis1_, ellipsis2_, removeNAValues_b_1&quot; #&gt; [5] &quot;ellipsis1_, ellipsis2_, ellipsis3_&quot; #&gt; [6] &quot;ellipsis1_, ellipsis2_, ellipsis3_, removeNAValues_b_1&quot; #&gt; #&gt; $ellipsis #&gt; [1] &quot;{homo,hetero}_{list}_{one,two,three}&quot; #&gt; #&gt; $default #&gt; [1] &quot;{none,full}&quot; All failures seems to be related to arguments passed as list. 10.5 A tricky example Let’s now use function kronecker from base package. 10.5.1 Create wrapper function As you know, we need first to create the offensive programming wrapper function. op_kronecker &lt;- opwf(kronecker, c(&#39;arrayA_a_1&#39;, &#39;arrayB_a_1&#39;, &#39;function_f_1&#39;, &#39;computeDimensionNames_b_1&#39;, &#39;...&#39;)) op_kronecker #&gt; function (arrayA_a_1, arrayB_a_1, function_f_1 = &quot;*&quot;, computeDimensionNames_b_1 = FALSE, #&gt; ...) #&gt; { #&gt; kronecker(arrayA_a_1, arrayB_a_1, FUN = function_f_1, make.dimnames = computeDimensionNames_b_1, #&gt; ...) #&gt; } #&gt; &lt;environment: 0x557d9acaef18&gt; 10.5.2 Getting some complexity knowledge How complex is it to test this function? cac_kronecker &lt;- computeArgumentsCombination(op_kronecker) print(cac_kronecker$signatures) #&gt; [[1]] #&gt; [1] &quot;arrayA_a_1, arrayB_a_1&quot; #&gt; #&gt; [[2]] #&gt; [1] &quot;arrayA_a_1, arrayB_a_1, ellipsis1_&quot; #&gt; #&gt; [[3]] #&gt; [1] &quot;arrayA_a_1, arrayB_a_1, ellipsis1_, ellipsis2_&quot; #&gt; #&gt; [[4]] #&gt; [1] &quot;arrayA_a_1, arrayB_a_1, ellipsis1_, ellipsis2_, ellipsis3_&quot; #&gt; #&gt; [[5]] #&gt; [1] &quot;arrayA_a_1, arrayB_a_1, computeDimensionNames_b_1&quot; #&gt; #&gt; [[6]] #&gt; [1] &quot;arrayA_a_1, arrayB_a_1, computeDimensionNames_b_1, ellipsis1_&quot; #&gt; #&gt; [[7]] #&gt; [1] &quot;arrayA_a_1, arrayB_a_1, computeDimensionNames_b_1, ellipsis1_, ellipsis2_&quot; #&gt; #&gt; [[8]] #&gt; [1] &quot;arrayA_a_1, arrayB_a_1, computeDimensionNames_b_1, ellipsis1_, ellipsis2_, ellipsis3_&quot; #&gt; #&gt; [[9]] #&gt; [1] &quot;arrayA_a_1, arrayB_a_1, function_f_1&quot; #&gt; #&gt; [[10]] #&gt; [1] &quot;arrayA_a_1, arrayB_a_1, function_f_1, ellipsis1_&quot; #&gt; #&gt; [[11]] #&gt; [1] &quot;arrayA_a_1, arrayB_a_1, function_f_1, ellipsis1_, ellipsis2_&quot; #&gt; #&gt; [[12]] #&gt; [1] &quot;arrayA_a_1, arrayB_a_1, function_f_1, ellipsis1_, ellipsis2_, ellipsis3_&quot; #&gt; #&gt; [[13]] #&gt; [1] &quot;arrayA_a_1, arrayB_a_1, function_f_1, computeDimensionNames_b_1&quot; #&gt; #&gt; [[14]] #&gt; [1] &quot;arrayA_a_1, arrayB_a_1, function_f_1, computeDimensionNames_b_1, ellipsis1_&quot; #&gt; #&gt; [[15]] #&gt; [1] &quot;arrayA_a_1, arrayB_a_1, function_f_1, computeDimensionNames_b_1, ellipsis1_, ellipsis2_&quot; #&gt; #&gt; [[16]] #&gt; [1] &quot;arrayA_a_1, arrayB_a_1, function_f_1, computeDimensionNames_b_1, ellipsis1_, ellipsis2_, ellipsis3_&quot; There are sixteen call signatures for which meta testing must generate data. 10.5.3 Firing massive tests Let’s try brut force analysis first. tryCatch(es &lt;- exploreSignatures(op_kronecker), error = function(e) print(e) ) #&gt; &lt;simpleError in abort(&quot;no draw function associated with&quot;, strBracket(value_s_1[1])): no draw function associated with [a]&gt; This fails as there exist no data generation function provided for array. We have to provide one by recording it into the data factory. To do so, here is the procedure to follow create the draw function that will generate the data type you need and that is not yet recorded into the data factory test it unitary and ensure result is of good type record new types into the data factory make your factory findable fire tests # step 1 - create the draw function - wrong way wrong_draw_integer_array &lt;- function(n, replace_b_1 = TRUE) { m &lt;- n + sample(0:3, 1) matrix(seq(1, n * m), byrow = TRUE, nrow = n, dimnames = list(paste(&#39;row_&#39;, 1:n), paste(&#39;col_&#39;, 1:m))) } # wrong because it does not respect argument names that must be # n_i_1 and replace_b_1 # step 2 - test draw function a1 &lt;- wrong_draw_integer_array(2) # step 3 - record new type into data factory - failure here df &lt;- retrieveDataFactory() df$addSuffix(&#39;a&#39;, &#39;array&#39;, wrong_draw_integer_array) #&gt; [1] FALSE # step 1 - create the draw factory - right way draw_integer_array &lt;- function(n_i_1, replace_b_1 = TRUE) { m &lt;- n_i_1 + sample(0:3, 1) matrix(seq(1, n_i_1 * m), byrow = TRUE, nrow = n_i_1, dimnames = list(paste(&#39;row_&#39;, 1:n_i_1), paste(&#39;col_&#39;, 1:m))) } # step 1 - create draw function draw_function &lt;- function(n_i_1, replace_b_1 = TRUE) { list(`*`, `+`, `-`)[[sample(1:3, 1)]]} # step - 2 test good practice verifies your functions behave correctly on a single example a1 &lt;- draw_integer_array(2) a2 &lt;- draw_integer_array(3) f &lt;- draw_function(1) kronecker(a1, a2, f, TRUE) #&gt; col_ 1:col_ 1 col_ 1:col_ 2 col_ 1:col_ 3 col_ 1:col_ 4 #&gt; row_ 1:row_ 1 1 2 3 4 #&gt; row_ 1:row_ 2 6 7 8 9 #&gt; row_ 1:row_ 3 11 12 13 14 #&gt; row_ 2:row_ 1 6 12 18 24 #&gt; row_ 2:row_ 2 36 42 48 54 #&gt; row_ 2:row_ 3 66 72 78 84 #&gt; col_ 1:col_ 5 col_ 2:col_ 1 col_ 2:col_ 2 col_ 2:col_ 3 #&gt; row_ 1:row_ 1 5 2 4 6 #&gt; row_ 1:row_ 2 10 12 14 16 #&gt; row_ 1:row_ 3 15 22 24 26 #&gt; row_ 2:row_ 1 30 7 14 21 #&gt; row_ 2:row_ 2 60 42 49 56 #&gt; row_ 2:row_ 3 90 77 84 91 #&gt; col_ 2:col_ 4 col_ 2:col_ 5 col_ 3:col_ 1 col_ 3:col_ 2 #&gt; row_ 1:row_ 1 8 10 3 6 #&gt; row_ 1:row_ 2 18 20 18 21 #&gt; row_ 1:row_ 3 28 30 33 36 #&gt; row_ 2:row_ 1 28 35 8 16 #&gt; row_ 2:row_ 2 63 70 48 56 #&gt; row_ 2:row_ 3 98 105 88 96 #&gt; col_ 3:col_ 3 col_ 3:col_ 4 col_ 3:col_ 5 col_ 4:col_ 1 #&gt; row_ 1:row_ 1 9 12 15 4 #&gt; row_ 1:row_ 2 24 27 30 24 #&gt; row_ 1:row_ 3 39 42 45 44 #&gt; row_ 2:row_ 1 24 32 40 9 #&gt; row_ 2:row_ 2 64 72 80 54 #&gt; row_ 2:row_ 3 104 112 120 99 #&gt; col_ 4:col_ 2 col_ 4:col_ 3 col_ 4:col_ 4 col_ 4:col_ 5 #&gt; row_ 1:row_ 1 8 12 16 20 #&gt; row_ 1:row_ 2 28 32 36 40 #&gt; row_ 1:row_ 3 48 52 56 60 #&gt; row_ 2:row_ 1 18 27 36 45 #&gt; row_ 2:row_ 2 63 72 81 90 #&gt; row_ 2:row_ 3 108 117 126 135 #&gt; col_ 5:col_ 1 col_ 5:col_ 2 col_ 5:col_ 3 col_ 5:col_ 4 #&gt; row_ 1:row_ 1 5 10 15 20 #&gt; row_ 1:row_ 2 30 35 40 45 #&gt; row_ 1:row_ 3 55 60 65 70 #&gt; row_ 2:row_ 1 10 20 30 40 #&gt; row_ 2:row_ 2 60 70 80 90 #&gt; row_ 2:row_ 3 110 120 130 140 #&gt; col_ 5:col_ 5 #&gt; row_ 1:row_ 1 25 #&gt; row_ 1:row_ 2 50 #&gt; row_ 1:row_ 3 75 #&gt; row_ 2:row_ 1 50 #&gt; row_ 2:row_ 2 100 #&gt; row_ 2:row_ 3 150 # step 3 - record new data types into data factory - success here df$addSuffix(&#39;a&#39;, &#39;array&#39;, draw_integer_array) #&gt; [1] TRUE df$addSuffix(&#39;f&#39;, &#39;function&#39;, draw_function) #&gt; [1] TRUE # step 4 - make your factory findable options(op_mt_data_factory = df) # step 5 - fire tests - up to 768 contexts managed in one shot es &lt;- exploreSignatures(op_kronecker) 10.5.3.1 Result analysis Computed results helps greatly analysis. nm &lt;- names(es$success$synthesis) sx &lt;- nm[startsWith(nm, &#39;number_suc&#39;)][1] # typo error number_sucessful corrected in v1.1.12 of metaTesting cat(&#39;succesful tests:&#39;, es$success$synthesis[[sx]], &#39;\\n&#39;) #&gt; succesful tests: 144 cat(&#39;erroneous tests:&#39;, es$failure$synthesis$number_erroneous_tests, &#39;\\n&#39;) #&gt; erroneous tests: 432 print(es$failure$synthesis$error) #&gt; [1] &quot;Error in outer(X, Y, FUN, ...): using ... with FUN = \\&quot;*\\&quot; is an error\\n&quot; #&gt; [2] &quot;Error in FUN(X, Y, ...): operator needs one or two arguments\\n&quot; Here number of successful test is already very significant, although overcame by number of failure tests. cat(&#39;succesful pattern:&#39;, es$success$synthesis$imperative, &#39;\\n&#39;) #&gt; succesful pattern: {homo,hetero}_{vector,list}_{one,two,three} cat(&#39;erroneous pattern:&#39;, es$failure$synthesis$imperative, &#39;\\n&#39;) #&gt; erroneous pattern: {homo,hetero}_{vector,list}_{one,two,three} Patterns for success and failure are the same. So, issue should logically come from data types. cat(&#39;succesful signatures\\n&#39;) #&gt; succesful signatures print(es$success$synthesis$signatures) #&gt; [1] &quot;arrayA_a_1, arrayB_a_1&quot; #&gt; [2] &quot;arrayA_a_1, arrayB_a_1, function_f_1, computeDimensionNames_b_1&quot; cat(&#39;erroneous signatures\\n&#39;) #&gt; erroneous signatures print(es$failure$synthesis$signatures) #&gt; [1] &quot;arrayA_a_1, arrayB_a_1, ellipsis1_&quot; #&gt; [2] &quot;arrayA_a_1, arrayB_a_1, function_f_1, computeDimensionNames_b_1, ellipsis1_&quot; #&gt; [3] &quot;arrayA_a_1, arrayB_a_1, ellipsis1_, ellipsis2_&quot; #&gt; [4] &quot;arrayA_a_1, arrayB_a_1, function_f_1, computeDimensionNames_b_1, ellipsis1_, ellipsis2_&quot; #&gt; [5] &quot;arrayA_a_1, arrayB_a_1, ellipsis1_, ellipsis2_, ellipsis3_&quot; #&gt; [6] &quot;arrayA_a_1, arrayB_a_1, function_f_1, computeDimensionNames_b_1, ellipsis1_, ellipsis2_, ellipsis3_&quot; Analysis of signatures brings more insight as all the error signatures implies ellipsis. Such result is great as it provides very quickly a good hint about the root cause. 10.6 Pitfalls to avoid Some common and well-known pitfalls are When using opwf function, make sure you provide the argument names in the right order. Examine created function signature prior going further. Make sure it fits the desired definition you look for. DataFactory changes remain invisible to processing as long as you do not set the option op_mt_data_factory with the name of the R variable that holds the DataFactory you want to use. This is often forgotten. "],
["generating-r-documentation.html", " 11 Generating R documentation 11.1 Conceptual approach 11.2 Vignettes 11.3 API 11.4 Best generation strategy 11.5 Pure R function 11.6 Offensive programming R function 11.7 Known limits 11.8 Opportunities", " 11 Generating R documentation A good practice of coding is producing related documentation. Easy to say, particularly annoying to achieve. Package wyz.code.rdoc aims to ease R documentation generation. Its works whatever the offensive programming instrumentation reached. In particular, under no instrumentation you can still generate R documentation. Using wyz.code.rdoc, manual pages .Rd files stored in man folder in a package context can be automatically created and filled up nearly to completion, depending on the level of offensive programming instrumentation of your code. Generated manual pages uses English language. Feel free to modify produced English, to match your own English flavor, if needed. The level of your R code instrumentation will impact the quality of the generation and your review work depth and time. When using both function return type instrumentation and test case instrumentation, expects produced manual page content to be fully generated and ready for review. When your code is not offensive programming instrumented, documentation generation is still possible. Thanks to customization as code, you may provide your customization to patch generated content. 11.1 Conceptual approach Package wyz.code.rdoc provides an API you can use to generate your manual pages. The approach followed is different from already well known documentation generation approaches, utils and roxygen2 approaches. Refer to vignette named tutorial of wyz.code.rdoc for a detailed explanations of approaches and comparison with wyz.code.rdoc. In short, while utils generates a documentation template for you to fill in with your documentation content, roxygen2 instruments comments in your code to generate on-demand the documentation by an automated process. In contrary wyz.code.rdoc uses code to generate documentation. The main benefit is that is allows you to customize the result by the code, thus offering a higher degree of freedom, higher reproducibility while saving time. 11.2 Vignettes Package wyz.code.rdoc provides a vignette named documentation that presents all vignettes of this package to grant an easy navigation. Please refer to this vignette and browse tutorial, use cases, and tips and tricks to know more about wyz.code.rdoc functionalities. 11.3 API Package wyz.code.rdoc allows to generate all kind of manual pages, in particular manual pages for data, functions, and R objects of various types environment, S3, S4, R6, RC. The API allows you to meet the real R specification as described in document Writing R extensions, by providing R functions to ease handling of manual pages markup language and of special required constructs. You may add missing sections, format presentations, x-ref other manual pages, add URLs, change phraseology and/or content as needed. wyz.code.rdoc provides a convenient and easy way to do so. Let’s see how through some examples. 11.4 Best generation strategy The best manual pages generation strategies are presented below from best to worst strategy name benefits comments full automated generation no hand edition, reproducibility, robustness, reusable not always applicable. See 11.7. partially automated generation hand edition is allowed Sometimes the shortest path. Hand modifications may be lost while regenerating content. Constrained reproducibility. Stick to full automated generation wherever possible. Also, keep it simple and stupid. It is possible to enter very deep complexity using wyz.code.rdoc, indeed good documentation generally requires good presentation and good wording. The later is generally forgotten and does not depends on any software piece. 11.5 Pure R function Let’s consider following R function named pure_r. library(wyz.code.rdoc) nop &lt;- function() {} pure_r &lt;- nop formals(pure_r) &lt;- alist(x = , y = NULL, z = FALSE) Now, let’s create its documentation as in package my_package, using wyz.code.rdoc. ic &lt;- InputContext(object = NULL, method = &#39;pure_r&#39;, package = &#39;my_package&#39;) pc &lt;- ProcessingContext(extraneous = list( concept = &#39;my concept&#39;, keyword = &#39;utils&#39; ) ) gc &lt;- GenerationContext(verbosity = TRUE, overwrite = TRUE) pmp &lt;- produceManualPage(ic, pc, gc) #&gt; #&gt; ------------------------------------------------------------------------------ #&gt; Creating manual page for function pure_r #&gt; standard section multi concept #&gt; standard section multi keyword #&gt; wrote file /tmp/RtmpgMcFnE/pure_r.Rd #&gt; filename is /tmp/RtmpgMcFnE/pure_r.Rd [OVERWRITTEN] #&gt; generated 9 sections: name, alias, title, usage, arguments, author, keyword, concept, encoding #&gt; missing 3 sections: description, value, examples #&gt; probably missing 1 section: details #&gt; replacements to manage: 3 #&gt; WARNING: File /tmp/RtmpgMcFnE/pure_r.Rd #&gt; checkRd: (5) /tmp/RtmpgMcFnE/pure_r.Rd:0-20: Must have a \\description First, notice that you passed the function name, not the function itself. Second, notice I set overwrite in order to be able to change file if it exists. Without this option, processing will take place but won’t be saved into the targeted file name. The result tells you what happened. By default generation uses folder /tmp. You may change this by providing setting argument /tmp of object GenerationContext. The provided results is oriented towards good documentation production. As we are documenting a function, sections descriptions, details, values and examples should be present. The warning shown comes from standard R documentation verification tool, namely tools::checkRd. This tool is use when checking package. You must get rid of errors, warnings and notes if you plan to publish your package. Let’s do it. examples &lt;- list( function() { pure_r(sum, 1:5) }, function() { pure_r(setenv) } ) pc &lt;- ProcessingContext(extraneous = list( description = &#39;tells if an R function is pure or not&#39;, details = &#39;A function is told to be pure if bla bla bla&#39;, value = &#39;A single boolean value&#39;, examples = convertExamples(examples, captureOutput = FALSE), concept = &#39;my concept&#39;, keyword = &#39;utils&#39; ) ) pmp2 &lt;- produceManualPage(ic, pc, gc) #&gt; #&gt; ------------------------------------------------------------------------------ #&gt; Creating manual page for function pure_r #&gt; standard section mono description #&gt; standard section mono details #&gt; standard section mono value #&gt; standard section mono examples #&gt; standard section multi concept #&gt; standard section multi keyword #&gt; wrote file /tmp/RtmpgMcFnE/pure_r.Rd #&gt; filename is /tmp/RtmpgMcFnE/pure_r.Rd [OVERWRITTEN] #&gt; generated 13 sections: name, alias, title, description, usage, arguments, details, value, author, examples, keyword, concept, encoding #&gt; replacements to manage: 6 #&gt; File /tmp/RtmpgMcFnE/pure_r.Rd passes standard documentation checks The generated manual page now passed the standard R documentation verification. Let’s have a look to generated file now. cat(paste(readLines(pmp2$context$filename, warn = FALSE), collapse = &#39;\\n&#39;)) #&gt; \\name{pure_r} #&gt; \\alias{pure_r} #&gt; \\title{Function pure_r} #&gt; \\description{ #&gt; tells if an R function is pure or not #&gt; } #&gt; \\usage{ #&gt; pure_r(x, y, z = FALSE) #&gt; } #&gt; \\arguments{ #&gt; \\item{x}{XXX_004} #&gt; \\item{y}{XXX_005} #&gt; \\item{z}{XXX_006} #&gt; } #&gt; \\details{ #&gt; A function is told to be pure if bla bla bla #&gt; } #&gt; \\value{ #&gt; A single boolean value #&gt; } #&gt; \\author{ #&gt; \\packageAuthor{my_package} #&gt; #&gt; Maintainer: \\packageMaintainer{my_package} #&gt; } #&gt; \\examples{ #&gt; # ------- example 1 ------- #&gt; pure_r(sum, 1:5) #&gt; #&gt; # ------- example 2 ------- #&gt; pure_r(setenv) #&gt; #&gt; } #&gt; \\keyword{utils} #&gt; \\concept{my concept} #&gt; \\encoding{UTF-8} As you see, some content as been generated with place holders prefixed by XXX. Although valid from a format point of view, this is meaningless, and should be corrected. You could fix it manually, but you will lose the ability to regenerate the manual page without losing some manual changes already made. Changing it by code keeps evolutivity and regeneration clean. Here is how to fix it, just use post processing. pc &lt;- ProcessingContext(extraneous = list( description = &#39;tells if an R function is pure or not&#39;, details = &#39;A function is told to be pure if bla bla bla&#39;, value = &#39;A single boolean value&#39;, examples = convertExamples(examples, captureOutput = FALSE), concept = &#39;my concept&#39;, keyword = &#39;utils&#39; ), postProcessing = list( arguments = function(content_s) { s &lt;- sub(&#39;XXX_007&#39;, sentensize(&#39;a typical description for variable x&#39;), content_s, fixed = TRUE) s &lt;- sub(&#39;XXX_008&#39;, sentensize(&#39;a typical description for variable y&#39;), s, fixed = TRUE) s &lt;- sub(&#39;XXX_009&#39;, sentensize(&#39;a typical description for variable z&#39;), s, fixed = TRUE) s } ) ) pmp3 &lt;- produceManualPage(ic, pc, gc) #&gt; #&gt; ------------------------------------------------------------------------------ #&gt; Creating manual page for function pure_r #&gt; standard section mono description #&gt; standard section mono details #&gt; standard section mono value #&gt; standard section mono examples #&gt; standard section multi concept #&gt; standard section multi keyword #&gt; patch arguments #&gt; wrote file /tmp/RtmpgMcFnE/pure_r.Rd #&gt; filename is /tmp/RtmpgMcFnE/pure_r.Rd [OVERWRITTEN] #&gt; generated 13 sections: name, alias, title, description, usage, arguments, details, value, author, examples, keyword, concept, encoding #&gt; patched 1 section: arguments #&gt; replacements to manage: 9 #&gt; File /tmp/RtmpgMcFnE/pure_r.Rd passes standard documentation checks cat(paste(readLines(pmp3$context$filename, warn = FALSE), collapse = &#39;\\n&#39;)) #&gt; \\name{pure_r} #&gt; \\alias{pure_r} #&gt; \\title{Function pure_r} #&gt; \\description{ #&gt; tells if an R function is pure or not #&gt; } #&gt; \\usage{ #&gt; pure_r(x, y, z = FALSE) #&gt; } #&gt; \\arguments{ #&gt; \\item{x}{A typical description for variable x.} #&gt; \\item{y}{A typical description for variable y.} #&gt; \\item{z}{A typical description for variable z.} #&gt; } #&gt; \\details{ #&gt; A function is told to be pure if bla bla bla #&gt; } #&gt; \\value{ #&gt; A single boolean value #&gt; } #&gt; \\author{ #&gt; \\packageAuthor{my_package} #&gt; #&gt; Maintainer: \\packageMaintainer{my_package} #&gt; } #&gt; \\examples{ #&gt; # ------- example 1 ------- #&gt; pure_r(sum, 1:5) #&gt; #&gt; # ------- example 2 ------- #&gt; pure_r(setenv) #&gt; #&gt; } #&gt; \\keyword{utils} #&gt; \\concept{my concept} #&gt; \\encoding{UTF-8} Note that I had to infer variable names for next generation, not just had to use previous ones! Now, generated manual pages looks acceptable, from a content and format point of view. From documentation quality point of view, understand-ability is insufficient according to me, due to genericity of phraseology. Use specific terminology to make things crystal clear and easy to understand for most of your readers. Now that you understood the global pattern, you may use immediately a pattern like the one used for the final case above, to get in one single shot a manual page from an R function. 11.6 Offensive programming R function Using an offensive programming instrumented function eases processing in comparison to previous case. Let’s see how op_r &lt;- nop formals(op_r) &lt;- alist(functionName_f_1 = , functionArguments_l = NULL, verbosityFlag_b_1 = FALSE) Now, let’s create its documentation as in package my_package, using wyz.code.rdoc. examples &lt;- list( function() { op_r(sum, 1:5) }, function() { op_r(setenv) } ) ic &lt;- InputContext(object = NULL, method = &#39;op_r&#39;, package = &#39;my_package&#39;) pc &lt;- ProcessingContext(extraneous = list( description = &#39;tells if an R function is pure or not&#39;, details = &#39;A function is told to be pure if bla bla bla&#39;, value = &#39;A single boolean value&#39;, examples = convertExamples(examples, captureOutput = FALSE), concept = &#39;my concept&#39;, keyword = &#39;utils&#39; ) ) gc &lt;- GenerationContext(verbosity = TRUE, overwrite = TRUE) omp &lt;- produceManualPage(ic, pc, gc) #&gt; #&gt; ------------------------------------------------------------------------------ #&gt; Creating manual page for function op_r #&gt; standard section mono description #&gt; standard section mono details #&gt; standard section mono value #&gt; standard section mono examples #&gt; standard section multi concept #&gt; standard section multi keyword #&gt; wrote file /tmp/RtmpgMcFnE/op_r.Rd #&gt; filename is /tmp/RtmpgMcFnE/op_r.Rd [OVERWRITTEN] #&gt; generated 13 sections: name, alias, title, description, usage, arguments, details, value, author, examples, keyword, concept, encoding #&gt; File /tmp/RtmpgMcFnE/op_r.Rd passes standard documentation checks cat(paste(readLines(omp$context$filename, warn = FALSE), collapse = &#39;\\n&#39;)) #&gt; \\name{op_r} #&gt; \\alias{op_r} #&gt; \\title{Function op_r} #&gt; \\description{ #&gt; tells if an R function is pure or not #&gt; } #&gt; \\usage{ #&gt; op_r(functionName_f_1, functionArguments_l, verbosityFlag_b_1 = FALSE) #&gt; } #&gt; \\arguments{ #&gt; \\item{functionName_f_1}{A single function value} #&gt; \\item{functionArguments_l}{An unconstrained list} #&gt; \\item{verbosityFlag_b_1}{A single boolean value} #&gt; } #&gt; \\details{ #&gt; A function is told to be pure if bla bla bla #&gt; } #&gt; \\value{ #&gt; A single boolean value #&gt; } #&gt; \\author{ #&gt; \\packageAuthor{my_package} #&gt; #&gt; Maintainer: \\packageMaintainer{my_package} #&gt; } #&gt; \\examples{ #&gt; # ------- example 1 ------- #&gt; op_r(sum, 1:5) #&gt; #&gt; # ------- example 2 ------- #&gt; op_r(setenv) #&gt; #&gt; } #&gt; \\keyword{utils} #&gt; \\concept{my concept} #&gt; \\encoding{UTF-8} As you can see, no more need to qualify arguments. Semantic naming is used to generate the content. If generated content does not match perfectly your need, you can still apply post processing in a somewhat similar or dissimilar way. For convenience, just know that you can easily generate argument section content using following approach dt &lt;- data.table(fields = letters[24:26], description = paste(&#39;a typical description for variable&#39;, letters[24:26])) sapply(seq_len(nrow(dt)), function(k) { generateMarkup(dt[k]$fields, &#39;item&#39;, dt[k]$description) }) #&gt; [1] &quot;\\\\item{x}{a typical description for variable x}&quot; #&gt; [2] &quot;\\\\item{y}{a typical description for variable y}&quot; #&gt; [3] &quot;\\\\item{z}{a typical description for variable z}&quot; Using such approach allows you to replace purely the content using a post processing scheme. 11.7 Known limits Generation of manual pages can be quite tricky. Whereas package wyz.code.rdoc alleviates greatly the burden, some pitfalls remain. Here they are Generated manual page might not respect the maximum line length required by R CMD check, and this tool will provide explicit information about noncompliance. To solve issue, just split the content by adding carriage return wherever required. Generated documentation is quite stereotyped. Inject your instructions to customize the result. 11.8 Opportunities Reuse can be made at several levels depending of your needs. Roughly speaking, you may aim for one of these 3 levels of customization just customize some textual information. Generate the pages using package wyz.code.rdoc and modify page contents manually is generally the best way to achieve this goal customize some manual pages sections. Generate the pages using package wyz.code.rdoc while providing some dedicated context information. Refer to previous examples, and look at variables starting with extraneous. They allow you to inject your customized content in targeted sections. If you seek for fully customized manual page generation, then you may use package (ref:rd) to create your own R generation scheme. That way you will get the benefit of starting launched, using high-level R documentation generation functions, and also get the ability to reuse and customized provided generation scheme. This package uses only R code, and so you could get insight and reuse any part of it. "],
["implementation-figures.html", " 12 Implementation figures 12.1 File statistics 12.2 Code statistics 12.3 Tests and coverage", " 12 Implementation figures As a reminder, package versions are stated in preamble. 12.1 File statistics R package R files size Rd files size wyz.code.offensiveProgramming 41 54 36 41 wyz.code.testthat 8 10 4 8 wyz.code.metaTesting 24 43 12 21 wyz.code.rdoc 48 72 35 59 All sizes expressed in kilobytes. 12.2 Code statistics R package exposed functions internal functions wyz.code.offensiveProgramming 34 6 wyz.code.testthat 5 3 wyz.code.metaTesting 10 17 wyz.code.rdoc 17 35 Internal functions counts only function related to topic, not all internal functions. 12.3 Tests and coverage R package number of tests natural coverage wyz.code.offensiveProgramming 450 99.21% - 7 lines not covered wyz.code.testthat 21 99.48% - 1 line not covered wyz.code.metaTesting 475 99.09% - 7 lines not covered wyz.code.rdoc 812 98.36% - 22 lines not covered Natural coverage is the coverage without any coverage instrumentation. No file is off the coverage. No function nor code line is off also. In such a context, computed coverage really tells the percentage of lines covered by tests. "],
["conclusion.html", " 13 Conclusion 13.1 Benefits of offensive programming 13.2 Concerns of offensive programming 13.3 Your feedback is welcome", " 13 Conclusion 13.1 Benefits of offensive programming Neither exhaustive nor limitative list. Main benefits are Applicable to new and legacy code Code instrumentation at the required level, according to your needs. No obligation to comply completely or to instrument completely. Transient or persistent approach allow to deal with code you own and code you do not own. Evaluation modes eases incremental work. Offensive programming type_checking_enforcement mode is complimentary of standard R evaluation mode, not contesting with it. Usable at build time, at test time, and at run time wherever and whenever needed. Reusable test cases, immediately available to replay. No need to read manual pages to run a test case. No need to type or copy/paste code to replay a test Generate directly testthat test cases, at the scope of offensive programming instrumented code. See wyz.code.testthat Generate offensive programming random data sets for testing purpose wyz.code.metaTesting Generates R documentation from code, offensive programming instrumented or not. Using wyz.code.rdoc API allows to customize and industrialize R documentation generation process Has a smooth learning curve. It is quite simple and intuitive, both from a programming point of view and from an end-user point of view Matches your scope. It is usable at design, build, and run stages. Most of all, offensive programming brings following value reduced code size, as many checks are no more necessary and shall no more be implemented higher developer’s productivity on R implementation, although earned time is varying greatly from function to function, depending of its complexity. I got more than 15% of time gain using offensive programming coding on several R package creations. Automated test case generation reduces greatly the burden of testthat content generation. Expect a productivity gain higher than 70% here. Documentation creation is now reduced in a great proportion, leaving just the review at your charge. Expect a productivity gain higher than 80% here. increased execution speed, due to reduced and simplified code. Again, many checks are no more necessary, and comparing, some traditional R code with offensive programming R code, will bring a clear value in favor of the second, as it tends not only to reduce the volume of code, but also to simplify your R code and to ease bug avoidance. The root cause of these two improvements is coming from type purity. 13.2 Concerns of offensive programming Again a neither exhaustive, nor limitative list Non standard evaluation is always tricky and difficult to understand and put correctly in action, due to the two evaluation paradigms that are different from traditional R logic. Offensive programming is to be used wherever R standard evaluation scheme appears too limited or too lazy. The two extraneous evaluation paradigms might bring runtime performance penalty, especially if you compare with standard R evaluation. Indeed, doing so is unfair, as it is not comparing apple to apple. Offensive programming adds two more kind of checks to decide on result compliance, that are simply unknown from R standard evaluation scheme. One piece of advice, be sure to have a clear idea of what you expect. In standard R, we are all used to get results without even asking ourselves any question about types, polymorphism, parameters and computed results. With offensive programming you must be able to answer clearly to such questions. 13.3 Your feedback is welcome Your feedback about package usefulness, package usage, and package extensions is welcome, as any improvement suggestion.Share them, this will really help me. I you wish to contribute to package development, just drop me an email. "],
["explicit-lege-feliciter.html", "Explicit - Lege feliciter", " Explicit - Lege feliciter "]
]
